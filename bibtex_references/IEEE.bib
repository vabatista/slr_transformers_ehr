@inproceedings{9587454,
abstract = {The electronic medical record (EMR) of liver cancer covers a large amount of key information, including the pathological stage of the tumor, the location of the tumor, and the size of the tumor, which helps doctors quickly understand the patient's condition and make diagnosis. However, conceptually complex and multi-terminological EMR makes doctors hard to retrieve useful information, which not only leads to low work efficiency, but also may miss the key information of pathological understanding. Named entity recognition (NER) technology can help doctors quickly screen out key entities and improve the efficiency of clinicians. In this study, we proposed two model structures (Damped Pointer Network and Dynamic Fusion) to improve the accuracy and recall rate of entity recognition. The model structures were well fitted the EMR with small size of samples and many technical terms in the form of uncommon words. Experiments showed that the F1-score (harmonic average of accuracy and recall rate) of the model structures proposed in this study was 98.56%, which was 2% higher than other frequently-used recognition technologies like BERT and BERT-CRF.},
author = {Zhang, Liang and Qin, Bin and Cai, Ren and Wang, Zhili},
booktitle = {2021 International Conference on Public Health and Data Science (ICPHDS)},
doi = {10.1109/ICPHDS53608.2021.00034},
isbn = {978-1-6654-2594-0},
keywords = {Bit error rate,Liver,Optimization method,Pathology},
month = {jul},
pages = {129--132},
publisher = {IEEE},
title = {{Named Entity Recognition of liver cancer data based on Damped Pointer Network and Dynamic Fusion}},
url = {https://ieeexplore.ieee.org/document/9587454/},
year = {2021}
}
@inproceedings{9565714,
abstract = {Accurate diagnosis plays an important role in the clinical decision-making process. To reduce diagnostic errors, many researchers develop models to assist physicians in predicting the most probable diagnoses during a patient's visit. In this paper, we focus on the development of the models for clinical diagnostic decision support using a real-world Electronic Health Record (EHR) dataset with 592,715 patient-visits. We propose a novel diagnosis prediction framework, which is based on Bidirectional Encoder Representations from Transformers (BERT), for the disease classification based on textual clinical notes and age information from EHR data, which differs from the previously proposed models by the way of the construction of the input representation by four special embeddings and the composition of the classification layer. We conduct the experiments on the task of multi-class classification for 1,987 diagnosis codes. The experimental results demonstrate the improved performance of our models compared to the baselines trained by the advanced text classification methods. We also investigate the influence of different types of clinical text and age information on the model performance.},
author = {Tang, Rui and Yao, Haishen and Zhu, Zhaowei and Sun, Xingzhi and Hu, Gang and Li, Yichong and Xie, Guotong},
booktitle = {2021 IEEE 9th International Conference on Healthcare Informatics (ICHI)},
doi = {10.1109/ICHI52183.2021.00055},
isbn = {978-1-6654-0132-6},
issn = {2575-2634},
keywords = {Decision making,Medical servic,Text categorization},
month = {aug},
pages = {311--319},
publisher = {IEEE},
title = {{Embedding Electronic Health Records to Learn BERT-based Models for Diagnostic Decision Support}},
url = {https://ieeexplore.ieee.org/document/9565714/},
year = {2021}
}
@inproceedings{9474713,
abstract = {Electronic health records (EHRs) contain patient-related information formed by structured and unstructured data, a valuable data source for Natural Language Processing (NLP) in the healthcare domain. The contextual word embeddings and Transformer-based models have proved their potential, reaching state-of-the-art for various NLP tasks. Although the performance for downstream NLP tasks with free-texts written in English has recently improved, less resource is available considering clinical texts and low-resource languages such as Portuguese. Our objective is to develop a Generative Pre-trained Transformer 2 (GPT-2) language model for Portuguese to support clinical and biomedical NLP tasks. We fine-tuned a generic Portuguese GPT-2 model to corpora of biomedical texts written in Portuguese, using transfer learning. We experimented on a public dataset, manually annotated for detecting patient fall, i.e., a classification task. Our in-domain GPT-2 model outperformed the generic Portuguese GPT-2 model by 3.43 in F1-score (weighted). Our preliminary results show that transfer learning with domain literature can benefit Portuguese biomedical NLP tasks, aligned with other languages' results.},
author = {Schneider, Elisa Terumi Rubel and de Souza, Joao Vitor Andrioli and Gumiel, Yohan Bonescki and Moro, Claudia and Paraiso, Emerson Cabrera},
booktitle = {2021 IEEE 34th International Symposium on Computer-Based Medical Systems (CBMS)},
doi = {10.1109/CBMS52027.2021.00056},
isbn = {978-1-6654-4121-6},
issn = {2372-9198},
keywords = {Biological system modeling,Comp,Electric potential},
month = {jun},
pages = {474--479},
publisher = {IEEE},
title = {{A GPT-2 Language Model for Biomedical Texts in Portuguese}},
url = {https://ieeexplore.ieee.org/document/9474713/},
year = {2021}
}
@article{9628010,
abstract = {Electronic Health Records include health-related information, among which there is text mentioning health conditions and diagnoses. Usually, text is also coded using appropriate terminologies and classifications. The act of coding is time consuming and prone to mistakes. Consequently, there is increasing demand for clinical text mining tools to help coding. In last few years Natural Language Processing (NLP) models has been shown to be effective in sentence-level tasks. Taking advantage from the transfer learning capabilities of those models, a number of biomedicine and health specific models have been also developed. However, also biomedical models can be seen as too general for some specific area like diagnostic expressions. In this paper, we describe a BERT model specialized on tasks related to diagnoses and health conditions. To obtain a disease-related language model, we created a pre-training corpora starting from ICD-11 entities, and enriched them with documents selected by querying PubMed and Wikipedia with entity names. Fine-tuning has been carried out towards three downstream tasks on two different datasets. Results show that our model, besides being trained on a much smaller corpora than state-of-the-art algorithms, leads to comparable or higher accuracy scores on all the considered tasks, in particular 97.53% accuracy on death certificate coding, and 81.32% on clinical document coding, which are both slightly higher than other models. To summarize the practical implications of our work, we pre-trained and fine-tuned a domain specific BERT model on a small corpora, with comparable or better performance than state-of-the-art models. This approach may also simplify the development of models for languages different from English, due to the minor quantity of data needed for training.},
author = {Roitero, Kevin and Portelli, Beatrice and Popescu, Mihai Horia and Mea, Vincenzo Della},
doi = {10.1109/ACCESS.2021.3131386},
issn = {2169-3536},
journal = {IEEE Access},
keywords = {Biological system modeling,Bit error,Task analysis},
pages = {159714--159723},
title = {{DiLBERT: Cheap Embeddings for Disease Related Medical NLP}},
url = {https://ieeexplore.ieee.org/document/9628010/},
volume = {9},
year = {2021}
}
@inproceedings{9017143,
abstract = {Named entity recognition is a fundamental task in natural language processing and many studies have done about it in recent decades. Previous word representation methods represent words as a single vector of multiple dimensions, which ignore the ambiguity of the character in Chinese. To solve this problem, we apply a BERT-BiLSTM-CRF model to Chinese electronic medical records named entity recognition in this paper. This model enhances the semantic representation of words by using BERT pre-trained language model, then we combine a BiLSTM network with CRF layer, and the word vector is used as the input for training. To evaluate the performance, we compare this model with several baseline models in CCKS 2017 datasets. Experimental results demonstrate that the BERT-BiLSTM-CRF model could achieve a better performance than the other baseline models.},
author = {Zhang, Wentao and Jiang, Shaohua and Zhao, Shan and Hou, Kai and Liu, Yang and Zhang, Li},
booktitle = {2019 12th International Conference on Intelligent Computation Technology and Automation (ICICTA)},
doi = {10.1109/ICICTA49267.2019.00043},
isbn = {978-1-7281-4284-5},
keywords = {Bit error rate,Electronic med,Hidden Markov models},
month = {oct},
pages = {166--169},
publisher = {IEEE},
title = {{A BERT-BiLSTM-CRF Model for Chinese Electronic Medical Records Named Entity Recognition}},
url = {https://ieeexplore.ieee.org/document/9017143/},
year = {2019}
}
@inproceedings{9565778,
abstract = {The roles of dietary supplement (DS) usage on disease progression of patients with cognitive impairments remain unclear. Transformed-based language models were trained to identify DS use status from clinical notes among patients with Alzheimer's disease and related dementias (ADRD). The best name entity recognition for DS achieved F1-score is 0.964 and the PubMed BERT based use status classifier achieved the weighted F1-score of 0.879. Integrating with DS use from medication table, we identified totally 125 unique DS among patients with mild cognitive impairment (MCI) only and 108 unique DS among patients who progressed to ADRD.},
author = {Zhou, Sicheng and Schutte, Dalton and Xing, Aiwen and Chen, Jiyang and Wolfson, Julian and He, Zhe and Yu, Fang and Zhang, Rui},
booktitle = {2021 IEEE 9th International Conference on Healthcare Informatics (ICHI)},
doi = {10.1109/ICHI52183.2021.00096},
isbn = {978-1-6654-0132-6},
issn = {2575-2634},
keywords = {Bit error rate,Conferences,Medical services,Transf},
month = {aug},
pages = {513--514},
publisher = {IEEE},
title = {{Identification of Dietary Supplement Use from Electronic Health Records Using Transformer-based Language Models}},
url = {https://ieeexplore.ieee.org/document/9565778/},
year = {2021}
}
@inproceedings{9565737,
abstract = {The predictive Intensive Care Unit (ICU) scoring system plays an important role in ICU management for its capability of predicting important outcomes, especially mortality. There are many scoring systems that have been developed and used in the ICU. These scoring systems are primarily based on the structured clinical data contained in the electronic health record (EHR), which may suffer the loss of the important clinical information contained in the narratives and images. In this work, we build a deep learning based survival prediction model with multimodality data to predict ICU-mortality. Four sets of features are investigated: (1) physiological measurements of Simplified Acute Physiology Score (SAPS) II, (2) common thorax diseases pre-defined by radiologists, (3) BERT-based text representations, and (4) chest X-ray image features. We use the Medical Information Mart for Intensive Care IV (MIMIC-IV) dataset to evaluate the proposed model. Our model achieves the average C- index of 0.7847 (95% confidence interval, 0.7625-0.8068), which substantially exceeds that of the baseline with SAPS-II features (0.7477 (0.7238-0.7716)). Ablation studies further demonstrate the contributions of pre-defined labels (2.12%), text features (2.68%), and image features (2.96%). Our model achieves a higher average C-index than the traditional machine learning methods under the same feature fusion setting, which suggests that the deep learning methods can outperform the traditional machine learning methods in ICU-mortality prediction. These results highlight the potential of deep learning models with multimodal information to enhance ICU-mortality prediction. We make our work publicly available at https://github.com/bionlplab/mimic-icu-mortality.},
author = {Lin, Mingquan and Wang, Song and Ding, Ying and Zhao, Lihui and Wang, Fei and Peng, Yifan},
booktitle = {2021 IEEE 9th International Conference on Healthcare Informatics (ICHI)},
doi = {10.1109/ICHI52183.2021.00088},
isbn = {978-1-6654-0132-6},
issn = {2575-2634},
keywords = {Deep learning,MIMICs,P,Predictive models,Radiology},
month = {aug},
pages = {497--498},
publisher = {IEEE},
title = {{An empirical study of using radiology reports and images to improve ICU-mortality prediction}},
url = {https://ieeexplore.ieee.org/document/9565737/},
year = {2021}
}
@inproceedings{9361169,
abstract = {If HIV-associated Neurocognitive Disorder (HAND) can be diagnosed and treated early, it may delay or reverse its pathological process and improve the survival rate of patients. At present, there is little statistical information about HAND, which is very disadvantageous to the prevention and treatment of HAND. Therefore, this paper synthetically uses deep learning models such as bidirectional LSTMs, conditional random fields and PCNN to carry out entity recognition and relationship extraction for text data, such as electronic medical record and medical community, to construct visual knowledge graph. Firstly, entity type and relation type are defined, and then multi-source data are fused, and then entity recognition of BIO annotated data sets is carried out by using the BERT-BiLSTM-CRF model. It is found that the effect of using the BERT pretraining model is better than word2vec; then, the neural network PCNN-Attention based on sentence level selective attention mechanism is used. It is found that the precision rate, recall rate and F1 value of the model are better than PCNN-ONE and PCNN-AVE models. Finally, the entity and entity relationship are visualized by using Neo4j graph database. In this experiment, the HAND related knowledge graph was constructed and visualized, which is helpful to the popularization of HAND related medical knowledge and the diagnosis of doctors, and it is helpful to the early detection of ANI, and plays an important role in delaying the pathology.},
author = {Sun, Di and Peng, Yang and Li, Hongjun},
booktitle = {2020 International Conference on Artificial Intelligence and Computer Engineering (ICAICE)},
doi = {10.1109/ICAICE51518.2020.00032},
isbn = {978-1-7281-9146-1},
keywords = {Deep learning,Knowledge engineering,Visualization},
month = {oct},
pages = {134--141},
publisher = {IEEE},
title = {{Construction of Knowledge Graph of HIV-associated Neurocognitive Disorders Syndrome based on Deep Learning}},
url = {https://ieeexplore.ieee.org/document/9361169/},
year = {2020}
}
@inproceedings{9537857,
abstract = {Medical named entity recognition (NER) is an important task of clinical natural language processing (NLP). It is a hot issue in intelligent medicine research. Recently, the proposed Lattice-LSTM model has demonstrated that incorporating information of words in character sequence into character-level Chinese NER has achieved new benchmark results on the Chinese datasets in multiple other fields. However, due to the lattice structure is dynamic and complex. These lattice-based models are difficult to fully use of the GPUs parallel computing, which limits its application. In this work, we propose a Well-Behaved Transformer (WB-Transformer) model for Chinese medical named entity recognition, using a high-performance encoding strategy to separately encode the character of Chinese electronic medical records (EMRs) and words corresponding to the character. This reduces the impact of word segmentation errors while obtaining the word boundary information, and makes full use information of characters and words for Chinese medical NER. Experimental on three Chinese medical entity recognition datasets show that our proposed model outperforms other methods.},
author = {Zhang, Zhichang and Qin, Xiaohui and Qiu, Yanlong and Liu, Dan},
booktitle = {2021 3rd International Conference on Natural Language Processing (ICNLP)},
doi = {10.1109/ICNLP52887.2021.00033},
isbn = {978-1-6654-1411-1},
keywords = {Computational modeling,Lattices,Parallel processin},
month = {mar},
pages = {162--167},
publisher = {IEEE},
title = {{Well-Behaved Transformer for Chinese Medical NER}},
url = {https://ieeexplore.ieee.org/document/9537857/},
year = {2021}
}
@inproceedings{8965823,
abstract = {As the generation and accumulation of massive electronic health records (EHR), how to effectively extract the valuable medical information from EHR has been a popular research topic. During the medical information extraction, named entity recognition (NER) is an essential natural language processing (NLP) task. This paper presents our efforts using neural network approaches for this task. Based on the Chinese EHR offered by CCKS 2019 and the Second Affiliated Hospital of Soochow University (SAHSU), several neural models for NER, including BiLSTM, have been compared, along with two pre-trained language models, word2vec and BERT. We have found that the BERT-BiLSTM-CRF model can achieve approximately 75% F1 score, which outperformed all other models during the tests.},
author = {Dai, Zhenjin and Wang, Xutao and Ni, Pin and Li, Yuming and Li, Gangmin and Bai, Xuming},
booktitle = {2019 12th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)},
doi = {10.1109/CISP-BMEI48845.2019.8965823},
isbn = {978-1-7281-4852-6},
keywords = {Bit error,Medical diagnostic imaging,Task analysis},
month = {oct},
pages = {1--5},
publisher = {IEEE},
title = {{Named Entity Recognition Using BERT BiLSTM CRF for Chinese Electronic Health Records}},
url = {https://ieeexplore.ieee.org/document/8965823/},
year = {2019}
}
@article{9648338,
abstract = {Relation extraction (RE) aims to extract relational facts from plain text, which is essential to the biomedical research field with the rapid growth of biomedical literature and generally large volumes of biomedicine-related text coming from various sources. Numerous annotated corpora and state-of-the-art models have been introduced in the past five years. However, there are no general guidelines about evaluating models on these corpora in single-and cross-domain settings with diverse entities and relation types. We aim to fill this gap for the task of detecting whether a relation holds between two biomedical entities given a text span. In this work, we present a fine-grained evaluation intended to perform a comparative evaluation of four biomedical benchmarks and understand the efficiency of state-of-the-art neural architectures based on Long Short-Term Memory (LSTM) with cross-attention and Bidirectional Encoder Representations from Transformers (BERT) for relation extraction across two main domains, namely scientific abstracts and electronic health records. We present a comparative evaluation of biomedical RE datasets, including the PHAEDRA, i2b2/VA, BC5CDR, and MADE corpora. Our evaluation of BioBERT and LSTM for binary classification shows significant divergence in in-domain and out-of-domain performance, finding an average drop in F1-measure of 34.2% for BioBERT. The cross-attention LSTM model developed in this work exhibits better cross-domain performance, with a drop of only 27.6% in F-measure.},
author = {Alimova, Ilseyar and Tutubalina, Elena and Nikolenko, Sergey I},
doi = {10.1109/ACCESS.2021.3135381},
issn = {2169-3536},
journal = {IEEE Access},
keywords = {bi,natural language processing,relation extraction},
pages = {1432--1439},
title = {{Cross-Domain Limitations of Neural Models on Biomedical Relation Classification}},
url = {https://ieeexplore.ieee.org/document/9648338/},
volume = {10},
year = {2022}
}
@inproceedings{8965108,
abstract = {The identification and mining of entities in Electronic Medical Record (EMR) plays an important role in medical diagnosis. This paper used the BioBERT model based on Google BERT model for automatic annotation of clinical problems, treatments and tests in EMR. This method firstly pre-trained BioBERT model on the corpus of medical related fields to convert the text into a numerical vector. Next, the BiLSTM-CRF model was used to train the processed vectors and finally complete the entity tagging. The I2B2 2010 challenge dataset was used in the experiment. The experimental results show that the method can obviously improve performance of named entity recognition (NER) for EMR. The F1 score of the experiment is 87.10%, which meet the needs of clinical system applications and can promote the study of clinical decision in the future.},
author = {Yu, Xin and Hu, Wenshen and Lu, Sha and Sun, Xiaoyan and Yuan, Zhenming},
booktitle = {2019 10th International Conference on Information Technology in Medicine and Education (ITME)},
doi = {10.1109/ITME.2019.00022},
isbn = {978-1-7281-3918-0},
issn = {2474-3828},
keywords = {Biological system modeling,Bit error rate,Context},
month = {aug},
pages = {49--52},
publisher = {IEEE},
title = {{BioBERT Based Named Entity Recognition in Electronic Medical Record}},
url = {https://ieeexplore.ieee.org/document/8965108/},
year = {2019}
}
@article{9430499,
abstract = {Automatic clinical coding is an essential task in the process of extracting relevant information from unstructured documents contained in electronic health records (EHRs). However, most research in the development of computer-based methods for clinical coding focuses on texts written in English due to the limited availability of medical linguistic resources in languages other than English. With nearly 500 million native speakers, there is a worldwide interest in processing healthcare texts in Spanish. In this study, we systematically analyzed transformer-based models for automatic clinical coding in Spanish. Using a transfer-learning-based approach, the three existing transformer architectures that support the Spanish language, namely, multilingual BERT (mBERT), BETO and XLM-RoBERTa (XLM-R), were first pretrained on a corpus of real-world oncology clinical cases with the goal of adapting transformers to the particularities of Spanish medical texts. The resulting models were fine-tuned on three distinct clinical coding tasks, following a multilabel sentence classification strategy. For each analyzed transformer, the domain-specific version outperformed the original general domain model across those tasks. Moreover, the combination of the developed strategy with an ensemble approach leveraging the predictive capacities of the three distinct transformers yielded the best obtained results, with MAP scores of 0.662, 0.544 and 0.884 on CodiEsp-D, CodiEsp-P and Cantemist-Coding shared tasks, which remarkably improved the previous state-of-the-art performance by 11.6%, 10.3% and 4.4%, respectively. We publicly release the mBERT, BETO and XLMR transformers adapted to the Spanish clinical domain at https://github.com/guilopgar/ClinicalCodingTransformerES, providing the clinical natural language processing community with advanced deep learning methods for performing medical coding and other tasks in the Spanish clinical domain.},
author = {Lopez-Garcia, Guillermo and Jerez, Jose M and Ribelles, Nuria and Alba, Emilio and Veredas, Francisco J},
doi = {10.1109/ACCESS.2021.3080085},
issn = {2169-3536},
journal = {IEEE Access},
keywords = {Adaptation models,Encoding,Neoplasms,Task analysis},
pages = {72387--72397},
title = {{Transformers for Clinical Coding in Spanish}},
url = {https://ieeexplore.ieee.org/document/9430499/},
volume = {9},
year = {2021}
}
@inproceedings{9602374,
abstract = {Chinese Electronic Medical Record Named Entity Recognition (CNER) is to identify and extract the entities related to medical and clinical practice from electronic medical records and classify them into pre-defined categories. In the past few years, deep learning methods have been applied to CNER and have achieved remarkable results, especially the BERT pre-training model. the BERT model can achieve good results, but the high model's training cost and slow inference speed are unbearable. In order to solve these problems, scholars use various methods to compress the BERT model, such as knowledge distillation and architecture adjustment. In this article, FastBERT is improved and applied to CNER. The sample adaptation mechanism of this model is used to pick up the inference speed. It is learned from experiments that this method can not only improve the reasoning speed of entity recognition, but also maintains good performance.},
author = {Tuo, Jianyong and Liu, Zhanzhan and Chen, Qing and Ma, Xin and Wang, Youqing},
booktitle = {2021 33rd Chinese Control and Decision Conference (CCDC)},
doi = {10.1109/CCDC52312.2021.9602374},
isbn = {978-1-6654-4089-9},
issn = {1948-9447},
keywords = {Adaptation models,Deep learning,Text reco,Training},
month = {may},
pages = {2405--2410},
publisher = {IEEE},
title = {{Chinese Electronic Medical Record Named Entity Recognition based on FastBERT method}},
url = {https://ieeexplore.ieee.org/document/9602374/},
year = {2021}
}
@inproceedings{9529801,
abstract = {The widely deployed of hospital information systems causes an explosive growth of the electronic medical records (EMRs). It makes the medical structured processing technologies become critical to find researchable data in the large medical dataset. However, the high quality structured processing is a challenging task, in particular due to the inherent complexity and polysemy of medical terminology. In this paper, we propose a novel approach to achieve the joint extraction of events in Chinese electronic medical records, which solves the problem of cascading error transmission in traditional models and the ambiguity of Chinese characters. We first use the Bi-directional Encoder Representation from Transformers(BERT) model to mine features from the preprocessed medical data; then based on the characteristics of Chinese, we use the Bi-directional Long Short-Term Memory(BILSTM) model to capture the semantic information of the context. The experiments were conducted on a real dataset. The F1 score of our model in the identification and classification tasks of event triggers and arguments is the highest, reaching 71.6, 68.1, 55.4 and 46.9, respectively, which proves the effectiveness of the proposed method.},
author = {Wang, Jingnan and Li, Jianqiang and Zhu, Zhichao and Zhao, Qing and Yu, Yang and Yang, Liyin and Xu, Chun},
booktitle = {2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)},
doi = {10.1109/COMPSAC51774.2021.00292},
isbn = {978-1-6654-2463-9},
issn = {0730-3157},
keywords = {Bidirectional,Bit error rate,Semantics,Terminology},
month = {jul},
pages = {1924--1929},
publisher = {IEEE},
title = {{Joint Extraction of Events in Chinese Electronic Medical Records}},
url = {https://ieeexplore.ieee.org/document/9529801/},
year = {2021}
}
@inproceedings{9565706,
abstract = {Diabetic retinopathy (DR) is a leading cause of blindness in American adults. If detected early, DR can be treated to preventing further damage causing blindness, therefore, early detection is very important for the treatment of DR. There is an increasing interest in developing Al technologies to help early detection of DR using electronic health records (EHR). The detailed diagnoses information documented in image reports is a valuable resource that could help detect lesions from the medical image, thus helping early detection of DR. In this study, we examined two state-of-the-art transformer-based natural language processing models, including BERT and RoBERTa, to extract DR-related concepts from clinical narratives. We identified four different categories of DR-related clinical concepts including lesions, eye parts, laterality, and severity, developed annotation guidelines, annotated a DR-corpus of 536 image reports, and trained four transformer-based NLP models for clinical concept extraction. The experimental results show that the BERT model pretrained with the MIMIC III dataset achieved the best strict/lenient F1-score of 0.9503 and 0.9645, respectively.},
author = {Yu, Zehao and Yang, Xi and Sweeting, Gianna L and Ma, Yinghan and Stolte, Skylar E and Fang, Ruogu and Wu, Yonghui},
booktitle = {2021 IEEE 9th International Conference on Healthcare Informatics (ICHI)},
doi = {10.1109/ICHI52183.2021.00089},
isbn = {978-1-6654-0132-6},
issn = {2575-2634},
keywords = {Bit error rate,Blindness,MIMICs,Medica,Retinopathy},
month = {aug},
pages = {499--500},
publisher = {IEEE},
title = {{Identify Diabetic Retinopathy-related Clinical Concepts Using Transformer-based Natural Language Processing Methods}},
url = {https://ieeexplore.ieee.org/document/9565706/},
year = {2021}
}
@inproceedings{9529670,
abstract = {The wide adoption of electronic medical record (EMR) systems causes rapid growth of medical and clinical data. It makes the medical named entity recognition (NER) technologies become critical to find useful patient information in the medical dataset. However, the medical terminologies usually have the characteristics of inherent complexity and ambiguity, it is difficult to capture context-dependency representations by supervision signal from a simple single layer structure model. In order to address this problem, this paper proposes a hybrid model based on stacked Bidirectional Long Short-Term Memory (BILSTM) for medical named entity recognition, which we call BSBC (BERT combined with stacked BILSTM and CRF). First, we use Bidirectional Encoder Representation from Transformers (BERT) to perform unsupervised learning on an unlabeled dataset to obtain character-level embeddings. Then, stacked BILSTM is utilized to obtain context-dependency representations through the multi hidden layers structure. Finally, Conditional Random Field (CRF) is used to predict sequence tags. The experiment results show that our method significantly outperforms the baseline methods, it serves as a strong alternative approach compared with traditional methods.},
author = {Zhu, Zhichao and Li, Jianqiang and Zhao, Qing and Wei, Yu-Chih and Jia, Yanhe},
booktitle = {2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)},
doi = {10.1109/COMPSAC51774.2021.00293},
isbn = {978-1-6654-2463-9},
issn = {0730-3157},
keywords = {Bit,Computational modeling,Conferences,Terminology},
month = {jul},
pages = {1930--1935},
publisher = {IEEE},
title = {{Medical named entity recognition of Chinese electronic medical records based on stacked Bidirectional Long Short-Term Memory}},
url = {https://ieeexplore.ieee.org/document/9529670/},
year = {2021}
}
@article{Meng2021,
abstract = {Advancements in machine learning algorithms have had a beneficial impact on representation learning, classification, and prediction models built using electronic health record (EHR) data. Effort has been put both on increasing models' overall performance as well as improving their interpretability, particularly regarding the decision-making process. In this study, we present a temporal deep learning model to perform bidirectional representation learning on EHR sequences with a transformer architecture to predict future diagnosis of depression. This model is able to aggregate five heterogenous and high-dimensional data sources from the EHR and process them in a temporal manner for chronic disease prediction at various prediction windows. We applied the current trend of pretraining and fine-tuning on EHR data to outperform the current state-of-the-art in chronic disease prediction, and to demonstrate the underlying relation between EHR codes in the sequence. The model generated the highest increases of precision-recall area under the curve (PRAUC) from 0.70 to 0.76 in depression prediction compared to the best baseline model. Furthermore, the self-attention weights in each sequence quantitatively demonstrated the inner relationship between various codes, which improved the model's interpretability. These results demonstrate the model's ability to utilize heterogeneous EHR data to predict depression while achieving high accuracy and interpretability, which may facilitate constructing clinical decision support systems in the future for chronic disease screening and early detection.},
author = {Meng, Yiwen and Speier, William and Ong, Michael K and Arnold, Corey W},
doi = {10.1109/JBHI.2021.3063721},
issn = {2168-2194},
journal = {IEEE Journal of Biomedical and Health Informatics},
keywords = {Algorithms,Depression,Electronic Health Records,Humans,Information Storage and Retrieval,Machine Learning,diagnosis},
language = {eng},
month = {aug},
number = {8},
pages = {3121--3129},
pmid = {33661740},
title = {{Bidirectional Representation Learning From Transformers Using Multimodal Electronic Health Record Data to Predict Depression}},
url = {https://ieeexplore.ieee.org/document/9369833/},
volume = {25},
year = {2021}
}
@inproceedings{9473700,
abstract = {Language Technology is an essential component of many Cyber-Physical Systems (CPSs) because specialized linguistic knowledge is indispensable to prevent fatal errors. We present the case of automatic identification of implant terms. The need of an automatic identification of implant terms spurs from safety reasons because patients who have an implant may or may be not submitted to Magnetic Resonance Imaging (MRI). Normally, MRI scans are safe. However, in some cases an MRI scan may not be recommended. It is important to know if a patient has an implant, because MRI scanning is incompatible with some implants. At present, the process of ascertain whether a patient could be at risk is lengthy, manual, and based on the specialized knowledge of medical staff. We argue that this process can be sped up, streamlined and become safer by sieving through patients' medical records. In this paper, we explore how to discover implant terms in electronic medical records (EMRs) written in Swedish with an unsupervised approach. To this aim we use BERT, a state-of-the-art deep learning algorithm based on pre-trained word embeddings. We observe that BERT discovers a solid proportion of terms that are indicative of implants.},
author = {Jerdhaf, Oskar and Santini, Marina and Lundberg, Peter and Karlsson, Anette and Jonsson, Arne},
booktitle = {2021 IEEE International Conference on Communications Workshops (ICC Workshops)},
doi = {10.1109/ICCWorkshops50388.2021.9473700},
isbn = {978-1-7281-9441-7},
issn = {2694-2941},
keywords = {Annotations,Magnetic resonance imaging,Terminology},
month = {jun},
pages = {1--6},
publisher = {IEEE},
title = {{Focused Terminology Extraction for CPSs The Case of "Implant Terms" in Electronic Medical Records}},
url = {https://ieeexplore.ieee.org/document/9473700/},
year = {2021}
}
@inproceedings{9581252,
abstract = {The privacy protection mechanism in the health context is becoming a crucial task given the exponential increase in the adoption of the Electronic Health Records (EHRs) all around the world. This kind of data can be used for medical investigation and research only if it is filtered out of all the so called Protected Health Information (PHI). This paper proposes a clinical de-identification system based on deep learning techniques for Named Entity Recognition and aimed at recognizing PHI entities to be replaced by surrogates in EHRs for anonymization purposes. This system is based on ELECTRA, a recent neural language model, and is enhanced through a sub-document level analysis aimed at grouping input sentences together, through a Sentences Grouping Factor (SGF), with the aim of broadening the representation context and consequently enhancing its ability to learn. This system was experimentally tested on the official dataset distributed in 2014 by Informatics for Integrating Biology & the Bedside research group, exhibiting superior performance compared to the state of the art in terms of detection at the category level, crucial for properly substituting PHI entities with surrogates. The effectiveness of the proposed system with respect to its components has been also confirmed by a further experimental analysis performed by substituting BERT language model in place of ELECTRA and varying SGF in accordance with limitations concerning the maximum input size for the language model used.},
author = {Catelli, Rosario and Gargiulo, Francesco and Damiano, Emanuele and Esposito, Massimo and {De Pietro}, Giuseppe},
booktitle = {2021 IEEE International Conference on Digital Health (ICDH)},
doi = {10.1109/ICDH52753.2021.00050},
isbn = {978-1-6654-1685-6},
keywords = {Analytical models,Conference,Deep learning,Privacy},
month = {sep},
pages = {266--275},
publisher = {IEEE},
title = {{Clinical de-identification using sub-document analysis and ELECTRA}},
url = {https://ieeexplore.ieee.org/document/9581252/},
year = {2021}
}
@inproceedings{9123057,
abstract = {Multi-label text classification, which tags a given plain text with the most relevant labels from a label space, is an important task in the natural language process. To diagnose diseases, clinical researchers use a machine-learning algorithm to do multi-label clinical text classification. However, conventional machine learning methods can neither capture deep semantic information nor the context of words strictly. Diagnostic information from the EHRs (Electronic Health Records) is mainly constructed by unstructured clinical free text which is an obstacle for clinical feature extraction. Moreover, feature engineering is time-consuming and labor-intensive. With the rapid development of deep learning, we apply neural network models to resolve this problem mentioned above. To favor multi-label classification on EHRs, we propose FAMLC-BERT (Feature-level Attention for Multi-label classification on BERT) to capture semantic features from different layers. The model uses feature-level attention with BERT to recognize the labels of EHRs. We empirically compared our model with other state-of-the-art models on real-world documents collected from the hospital. Experiments show that our model achieved significant improvements compared to other selected benchmarks.},
author = {Pan, Disheng and Zheng, Xizi and Liu, Weijie and Li, Mengya and Ma, Meng and Zhou, Ying and Yang, Li and Wang, Ping},
booktitle = {2020 IEEE 6th Intl Conference on Big Data Security on Cloud (BigDataSecurity), IEEE Intl Conference on High Performance and Smart Computing, (HPSC) and IEEE Intl Conference on Intelligent Data and Security (IDS)},
doi = {10.1109/BigDataSecurity-HPSC-IDS49724.2020.00042},
isbn = {978-1-7281-6873-9},
keywords = {Clinical Text,Multi-label Text Classification,Pred},
month = {may},
pages = {186--191},
publisher = {IEEE},
title = {{Multi-label Classification for Clinical Text with Feature-level Attention}},
url = {https://ieeexplore.ieee.org/document/9123057/},
year = {2020}
}
@inproceedings{9313224,
abstract = {There are significant variabilities in clinicians' guideline-concordant documentation in asthma care. However, assessing clinicians' documentation is not feasible using only structured data but requires labor intensive chart review of electronic health records. Although the national asthma guidelines are available it is still challenging to use them as a real-time tool for providing feedback on adhering documentation guidelines for asthma care improvement. A certain guideline element, such as teaching or reviewing inhaler techniques, is difficult to capture by handcrafted rules since it requires contextual understanding of clinical narratives. This study examined a deep learning based natural language model, Bidirectional Encoder Representations from Transformers (BERT) coupled with distant supervision to identify inhaler techniques from clinical narratives. The BERT model with distant supervision outperformed the rule-based approach and achieved performance gain compared with the BERT without distant supervision.},
author = {Kshatriya, Bhavani Singh Agnikula and Sagheb, Elham and Wi, Chung-II and Yoon, Jungwon and Seol, Hee Yun and Juhn, Young and Sohn, Sunghwan},
booktitle = {2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)},
doi = {10.1109/BIBM49941.2020.9313224},
isbn = {978-1-7281-6215-7},
keywords = {Bit error rate,Data models,Guid,Respiratory system},
month = {dec},
pages = {1736--1739},
publisher = {IEEE},
title = {{Deep Learning Identification of Asthma Inhaler Techniques in Clinical Notes}},
url = {https://ieeexplore.ieee.org/document/9313224/},
year = {2020}
}
@inproceedings{9037721,
abstract = {This paper proposes a novel method based on the language representation model called BERT (Bidirectional Encoder Representations from Transformers) for Obstetric assistant diagnosis on Chinese obstetric EMRs (Electronic Medical Records). To aggregate more information for final output, an enhanced layer is augmented to the BERT model. In particular, the enhanced layer in this paper is constructed based on strategy 1(A strategy) and/or strategy 2(A-AP strategy). The proposed method is evaluated on two datasets including Chinese Obstetric EMRs dataset and Arxiv Academic Paper Dataset (AAPD). The experimental results show that the proposed method based on BERT improves the F1 value by 19.58% and 2.71% over the state-of-the-art methods, and the proposed method based on BERT and the enhanced layer by strategy 2 improves the F1 value by 0.7% and 0.3% (strategy 1 improves the F1 value by 0.68% and 0.1%) over the method without adding enhanced layer respectively on Obstetric EMRs dataset and AAPD dataset.},
author = {Zhang, Kunli and Liu, Chuang and Duan, Xuemin and Zhou, Lijuan and Zhao, Yueshu and Zan, Hongying},
booktitle = {2019 International Conference on Asian Language Processing (IALP)},
doi = {10.1109/IALP48816.2019.9037721},
isbn = {978-1-7281-5014-7},
keywords = {Bit error rate,Encoding,Medical diag,Task analysis},
month = {nov},
pages = {384--389},
publisher = {IEEE},
title = {{BERT with Enhanced Layer for Assistant Diagnosis Based on Chinese Obstetric EMRs}},
url = {https://ieeexplore.ieee.org/document/9037721/},
year = {2019}
}
@inproceedings{9562815,
abstract = {Effective code assignment for patient clinical records in a hospital plays a significant role in the process of standardizing medical records, mainly for streamlining clinical care delivery, billing, and managing insurance claims. The current practice employed is manual coding, usually carried out by trained medical coders, making the process subjective, error-prone, inexact, and time-consuming. To alleviate this cost-intensive process, intelligent coding systems built on patients' structured electronic medical records are critical. Classification of medical diagnostic codes, like ICD-10, is widely employed to categorize patients' clinical conditions and associated diagnoses. In this work, we present a neural model $\mathcal{LATA}$, built on Label Attention Transformer Architectures for automatic assignment of ICD-10 codes. Our work is benchmarked on the CodiEsp dataset, a dataset for automatic clinical coding systems for multilingual medical documents, used in the eHealth CLEF 2020-Multilingual Information Extraction Shared Task. The experimental results reveal that the proposed $\mathcal{LATA}$ variants outperform their basic BERT counterparts by 33-49% in terms of standard metrics like precision, recall, F1-score and mean average precision. The label attention mechanism also enables direct extraction of textual evidence in medical documents that map to the clinical ICD-10 diagnostic codes.},
author = {Mayya, Veena and Kamath, S Sowmya and Sugumaran, Vijayan},
booktitle = {2021 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB)},
doi = {10.1109/CIBCB49929.2021.9562815},
isbn = {978-1-6654-0112-8},
keywords = {Codes,Computer architecture,Manuals,Measurement,Pr},
month = {oct},
pages = {1--7},
publisher = {IEEE},
title = {{$\mathcal{LATA}{-}$ Label Attention Transformer Architectures for ICD-10 Coding of Unstructured Clinical Notes}},
url = {https://ieeexplore.ieee.org/document/9562815/},
year = {2021}
}
@article{Darabi2020,
abstract = {Effective representation learning of electronic health records is a challenging task and is becoming more important as the availability of such data is becoming pervasive. The data contained in these records are irregular and contain multiple modalities such as notes, and medical codes. They are preempted by medical conditions the patient may have, and are typically recorded by medical staff. Accompanying codes are notes containing valuable information about patients beyond the structured information contained in electronic health records. We use transformer networks and the recently proposed BERT language model to embed these data streams into a unified vector representation. The presented approach effectively encodes a patient's visit data into a single a distributed representation, which can be used for downstream tasks. Our model demonstrates superior performance and generalization on mortality, readmission and length of stay tasks using the publicly available MIMIC-III ICU dataset.},
author = {Darabi, Sajad and Kachuee, Mohammad and Fazeli, Shayan and Sarrafzadeh, Majid},
doi = {10.1109/JBHI.2020.2984931},
issn = {2168-2194},
journal = {IEEE Journal of Biomedical and Health Informatics},
keywords = {Electronic Health Records,Humans,Machine Learning,Natural Language Processing},
language = {eng},
month = {nov},
number = {11},
pages = {3268--3275},
pmid = {32287023},
title = {{TAPER: Time-Aware Patient EHR Representation}},
url = {https://ieeexplore.ieee.org/document/9056492/},
volume = {24},
year = {2020}
}
@inproceedings{9562819,
abstract = {Drug representations have played an important role in cheminformatics. However, in the healthcare domain, drug representations have been underused relative to the rest of Electronic Health Record (EHR) data, due to the complexity of high dimensional drug representations and the lack of proper pipeline that will allow to convert clinical drugs to their representations. Time-varying vital signs, laboratory measurements, and related time-series signals are commonly used to predict clinical outcomes. In this work, we demonstrated that using clinical drug representations in addition to other clinical features has significant potential to increase the performance of mortality and length of stay (LOS) models. We evaluate the two different drug representation methods (Extended -Connectivity Fingerprint- ECFP and SMILES-Transformer embedding) on clinical outcome predictions. The results have shown that the proposed multimodal approach achieves substantial enhancement on clinical tasks over baseline models. U sing clinical drug representations as additional features improve the LOS prediction for Area Under the Receiver Operating Characteristics (AUROC) around %6 and for Area Under Precision-Recall Curve (AUPRC) by around % 5. Furthermore, for the mortality prediction task, there is an improvement of around % 2 over the time series baseline in terms of AUROC and %3.5 in terms of AUPRC. The code for the proposed method is available at https://github.com/tanlab/MIMIC-III-Clinical-Drug-Representations.},
author = {Bardak, Batuhan and Tan, Mehmet},
booktitle = {2021 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB)},
doi = {10.1109/CIBCB49929.2021.9562819},
isbn = {978-1-6654-0112-8},
keywords = {Conferences,Drugs,Pipelines,R,Time series analysis},
month = {oct},
pages = {1--8},
publisher = {IEEE},
title = {{Using Clinical Drug Representations for Improving Mortality and Length of Stay Predictions}},
url = {https://ieeexplore.ieee.org/document/9562819/},
year = {2021}
}
