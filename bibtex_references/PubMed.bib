@article{Meng2021,
abstract = {Advancements in machine learning algorithms have had a beneficial impact on representation learning, classification, and prediction models built using electronic health record (EHR) data. Effort has been put both on increasing models' overall performance as well as improving their interpretability, particularly regarding the decision-making process. In this study, we present a temporal deep learning model to perform bidirectional representation learning on EHR sequences with a transformer architecture to predict future diagnosis of depression. This model is able to aggregate five heterogenous and high-dimensional data sources from the EHR and process them in a temporal manner for chronic disease prediction at various prediction windows. We applied the current trend of pretraining and fine-tuning on EHR data to outperform the current state-of-the-art in chronic disease prediction, and to demonstrate the underlying relation between EHR codes in the sequence. The model generated the highest increases of precision-recall area under the curve (PRAUC) from 0.70 to 0.76 in depression prediction compared to the best baseline model. Furthermore, the self-attention weights in each sequence quantitatively demonstrated the inner relationship between various codes, which improved the model's interpretability. These results demonstrate the model's ability to utilize heterogeneous EHR data to predict depression while achieving high accuracy and interpretability, which may facilitate constructing clinical decision support systems in the future for chronic disease screening and early detection.},
author = {Meng, Yiwen and Speier, William and Ong, Michael K and Arnold, Corey W},
doi = {10.1109/JBHI.2021.3063721},
issn = {2168-2194},
journal = {IEEE Journal of Biomedical and Health Informatics},
keywords = {Algorithms,Depression,Electronic Health Records,Humans,Information Storage and Retrieval,Machine Learning,diagnosis},
language = {eng},
month = {aug},
number = {8},
pages = {3121--3129},
pmid = {33661740},
title = {{Bidirectional Representation Learning From Transformers Using Multimodal Electronic Health Record Data to Predict Depression}},
url = {https://ieeexplore.ieee.org/document/9369833/},
volume = {25},
year = {2021}
}
@inproceedings{9313224,
abstract = {There are significant variabilities in clinicians' guideline-concordant documentation in asthma care. However, assessing clinicians' documentation is not feasible using only structured data but requires labor intensive chart review of electronic health records. Although the national asthma guidelines are available it is still challenging to use them as a real-time tool for providing feedback on adhering documentation guidelines for asthma care improvement. A certain guideline element, such as teaching or reviewing inhaler techniques, is difficult to capture by handcrafted rules since it requires contextual understanding of clinical narratives. This study examined a deep learning based natural language model, Bidirectional Encoder Representations from Transformers (BERT) coupled with distant supervision to identify inhaler techniques from clinical narratives. The BERT model with distant supervision outperformed the rule-based approach and achieved performance gain compared with the BERT without distant supervision.},
author = {Kshatriya, Bhavani Singh Agnikula and Sagheb, Elham and Wi, Chung-II and Yoon, Jungwon and Seol, Hee Yun and Juhn, Young and Sohn, Sunghwan},
booktitle = {2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)},
doi = {10.1109/BIBM49941.2020.9313224},
isbn = {978-1-7281-6215-7},
keywords = {Bit error rate,Data models,Guid,Respiratory system},
month = {dec},
pages = {1736--1739},
publisher = {IEEE},
title = {{Deep Learning Identification of Asthma Inhaler Techniques in Clinical Notes}},
url = {https://ieeexplore.ieee.org/document/9313224/},
year = {2020}
}
@article{Darabi2020,
abstract = {Effective representation learning of electronic health records is a challenging task and is becoming more important as the availability of such data is becoming pervasive. The data contained in these records are irregular and contain multiple modalities such as notes, and medical codes. They are preempted by medical conditions the patient may have, and are typically recorded by medical staff. Accompanying codes are notes containing valuable information about patients beyond the structured information contained in electronic health records. We use transformer networks and the recently proposed BERT language model to embed these data streams into a unified vector representation. The presented approach effectively encodes a patient's visit data into a single a distributed representation, which can be used for downstream tasks. Our model demonstrates superior performance and generalization on mortality, readmission and length of stay tasks using the publicly available MIMIC-III ICU dataset.},
author = {Darabi, Sajad and Kachuee, Mohammad and Fazeli, Shayan and Sarrafzadeh, Majid},
doi = {10.1109/JBHI.2020.2984931},
issn = {2168-2194},
journal = {IEEE Journal of Biomedical and Health Informatics},
keywords = {Electronic Health Records,Humans,Machine Learning,Natural Language Processing},
language = {eng},
month = {nov},
number = {11},
pages = {3268--3275},
pmid = {32287023},
title = {{TAPER: Time-Aware Patient EHR Representation}},
url = {https://ieeexplore.ieee.org/document/9056492/},
volume = {24},
year = {2020}
}
@article{Kim2020,
abstract = {BACKGROUND: While clinical entity recognition mostly aims at electronic health records (EHRs), there are also the demands of dealing with the other type of text data. Automatic medical diagnosis is an example of new applications using a different data source. In this work, we are interested in extracting Korean clinical entities from a new medical dataset, which is completely different from EHRs. The dataset is collected from an online QA site for medical diagnosis. Bidirectional Encoder Representations from Transformers (BERT), which is one of the best language representation models, is used to extract the entities. RESULTS: A slightly modified version of BERT labeling strategy replaces the original labeling to enhance the separation of postpositions in Korean. A new clinical entity recognition dataset that we construct, as well as a standard NER dataset, have been used for the experiments. A pre-trained multilingual BERT model is used for the initialization of the entity recognition model. BERT significantly outperforms a character-level bidirectional LSTM-CRF, a benchmark model, in terms of all metrics. The micro-averaged precision, recall, and f1 of BERT are 0.83, 0.85 and 0.84, whereas that of bi-LSTM-CRF are 0.82, 0.79 and 0.81 respectively. The recall values of BERT are especially better than that of the other model. It can be interpreted that the trained BERT model could detect out of vocabulary (OOV) words better than bi-LSTM-CRF. CONCLUSIONS: The recently developed BERT and its WordPiece tokenization are effective for the Korean clinical entity recognition. The experiments using a new dataset constructed for the purpose and a standard NER dataset show the superiority of BERT compared to a state-of-the-art method. To the best of our knowledge, this work is one of the first studies dealing with clinical entity extraction from non-EHR data.},
author = {Kim, Young-Min and Lee, Tae-Hoon},
doi = {10.1186/s12911-020-01241-8},
issn = {1472-6947},
journal = {BMC Medical Informatics and Decision Making},
keywords = {Computer,Electronic Health Records,Humans,Information Storage and Retrieval,Language,Machine Learning,Natural Language Processing,Neural Networks,Republic of Korea},
language = {eng},
month = {sep},
number = {S7},
pages = {242},
pmid = {32998724},
title = {{Korean clinical entity recognition from diagnosis text using BERT}},
url = {https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-01241-8},
volume = {20},
year = {2020}
}
@article{Mitra2021,
abstract = {BACKGROUND: Accurate detection of bleeding events from electronic health records (EHRs) is crucial for identifying and characterizing different common and serious medical problems. To extract such information from EHRs, it is essential to identify the relations between bleeding events and related clinical entities (eg, bleeding anatomic sites and lab tests). With the advent of natural language processing (NLP) and deep learning (DL)-based techniques, many studies have focused on their applicability for various clinical applications. However, no prior work has utilized DL to extract relations between bleeding events and relevant entities. OBJECTIVE: In this study, we aimed to evaluate multiple DL systems on a novel EHR data set for bleeding event-related relation classification. METHODS: We first expert annotated a new data set of 1046 deidentified EHR notes for bleeding events and their attributes. On this data set, we evaluated three state-of-the-art DL architectures for the bleeding event relation classification task, namely, convolutional neural network (CNN), attention-guided graph convolutional network (AGGCN), and Bidirectional Encoder Representations from Transformers (BERT). We used three BERT-based models, namely, BERT pretrained on biomedical data (BioBERT), BioBERT pretrained on clinical text (Bio+Clinical BERT), and BioBERT pretrained on EHR notes (EhrBERT). RESULTS: Our experiments showed that the BERT-based models significantly outperformed the CNN and AGGCN models. Specifically, BioBERT achieved a macro F1 score of 0.842, outperforming both the AGGCN (macro F1 score, 0.828) and CNN models (macro F1 score, 0.763) by 1.4% (P<.001) and 7.9% (P<.001), respectively. CONCLUSIONS: In this comprehensive study, we explored and compared different DL systems to classify relations between bleeding events and other medical concepts. On our corpus, BERT-based models outperformed other DL models for identifying the relations of bleeding-related entities. In addition to pretrained contextualized word representation, BERT-based models benefited from the use of target entity representation over traditional sequence representation.},
author = {Mitra, Avijit and Rawat, Bhanu Pratap Singh and McManus, David D and Yu, Hong},
doi = {10.2196/27527},
issn = {2291-9694},
journal = {JMIR Medical Informatics},
language = {eng},
month = {jul},
number = {7},
pages = {e27527},
pmid = {34255697},
title = {{Relation Classification for Bleeding Events From Electronic Health Records Using Deep Learning Systems: An Empirical Study}},
url = {https://medinform.jmir.org/2021/7/e27527},
volume = {9},
year = {2021}
}
@article{Gong2020,
abstract = {Background. Clinical named entity recognition is the basic task of mining electronic medical records text, which are with some challenges containing the language features of Chinese electronic medical records text with many compound entities, serious missing sentence components, and unclear entity boundary. Moreover, the corpus of Chinese electronic medical records is difficult to obtain. Methods. Aiming at these characteristics of Chinese electronic medical records, this study proposed a Chinese clinical entity recognition model based on deep learning pretraining. The model used word embedding from domain corpus and fine-tuning of entity recognition model pretrained by relevant corpus. Then BiLSTM and Transformer are, respectively, used as feature extractors to identify four types of clinical entities including diseases, symptoms, drugs, and operations from the text of Chinese electronic medical records. Results. 75.06% Macro-P, 76.40% Macro-R, and 75.72% Macro-F1 aiming at test dataset could be achieved. These experiments show that the Chinese clinical entity recognition model based on deep learning pretraining can effectively improve the recognition effect. Conclusions. These experiments show that the proposed Chinese clinical entity recognition model based on deep learning pretraining can effectively improve the recognition performance.},
author = {Gong, Lejun and Zhang, Zhifei and Chen, Shiqi},
doi = {10.1155/2020/8829219},
editor = {Yao, Jiafeng},
issn = {2040-2309},
journal = {Journal of Healthcare Engineering},
keywords = {China,Deep Learning,Electronic Health Records,Humans,Language,Natural Language Processing},
language = {eng},
month = {nov},
pages = {1--8},
pmid = {33299537},
title = {{Clinical Named Entity Recognition from Chinese Electronic Medical Records Based on Deep Learning Pretraining}},
url = {https://www.hindawi.com/journals/jhe/2020/8829219/},
volume = {2020},
year = {2020}
}
@article{Chen2021,
abstract = {BACKGROUND: The electronic health record (EHR) contains a wealth of medical information. An organized EHR can greatly help doctors treat patients. In some cases, only limited patient information is collected to help doctors make treatment decisions. Because EHRs can serve as a reference for this limited information, doctors' treatment capabilities can be enhanced. Natural language processing and deep learning methods can help organize and translate EHR information into medical knowledge and experience. OBJECTIVE: In this study, we aimed to create a model to extract concept embeddings from EHRs for disease pattern retrieval and further classification tasks. METHODS: We collected 1,040,989 emergency department visits from the National Taiwan University Hospital Integrated Medical Database and 305,897 samples from the National Hospital and Ambulatory Medical Care Survey Emergency Department data. After data cleansing and preprocessing, the data sets were divided into training, validation, and test sets. We proposed a Transformer-based model to embed EHRs and used Bidirectional Encoder Representations from Transformers (BERT) to extract features from free text and concatenate features with structural data as input to our proposed model. Then, Deep InfoMax (DIM) and Simple Contrastive Learning of Visual Representations (SimCLR) were used for the unsupervised embedding of the disease concept. The pretrained disease concept-embedding model, named EDisease, was further finetuned to adapt to the critical care outcome prediction task. We evaluated the performance of embedding using t-distributed stochastic neighbor embedding (t-SNE) to perform dimension reduction for visualization. The performance of the finetuned predictive model was evaluated against published models using the area under the receiver operating characteristic (AUROC). RESULTS: The performance of our model on the outcome prediction had the highest AUROC of 0.876. In the ablation study, the use of a smaller data set or fewer unsupervised methods for pretraining deteriorated the prediction performance. The AUROCs were 0.857, 0.870, and 0.868 for the model without pretraining, the model pretrained by only SimCLR, and the model pretrained by only DIM, respectively. On the smaller finetuning set, the AUROC was 0.815 for the proposed model. CONCLUSIONS: Through contrastive learning methods, disease concepts can be embedded meaningfully. Moreover, these methods can be used for disease retrieval tasks to enhance clinical practice capabilities. The disease concept model is also suitable as a pretrained model for subsequent prediction tasks.},
author = {Chen, Yen-Pin and Lo, Yuan-Hsun and Lai, Feipei and Huang, Chien-Hua},
doi = {10.2196/25113},
issn = {1438-8871},
journal = {Journal of Medical Internet Research},
keywords = {Adult,Algorithms,Electronic Health Records,Female,Humans,Information Storage and Retrieval,Male,Natural Language Processing,methods,standards},
language = {eng},
month = {jan},
number = {1},
pages = {e25113},
pmid = {33502324},
title = {{Disease Concept-Embedding Based on the Self-Supervised Method for Medical Information Extraction from Electronic Health Records and Disease Retrieval: Algorithm Development and Validation Study}},
url = {http://www.jmir.org/2021/1/e25113/},
volume = {23},
year = {2021}
}
@article{Liang2021,
abstract = {MOTIVATION: Medical terminology normalization aims to map the clinical mention to terminologies coming from a knowledge base, which plays an important role in analyzing Electronic Health Record (EHR) and many downstream tasks. In this paper, we focus on Chinese procedure terminology normalization. The expressions of terminology are various and one medical mention may be linked to multiple terminologies. Existing studies based on Learning To Rank (LTR) does not fully consider the quality of negative samples during model training and the importance of keywords in this domain-specific task. RESULTS: We propose a combined recall and rank framework to solve these problems. A pair-wise Bert model with deep metric learning is used to recall candidates. Previous methods either train Bert in a point-wise way or based on a multi-class classification problem, which may lead serious efficiency problems or not be effective enough. During model training, we design a novel online negative sampling algorithm to activate the pair-wise method. To deal with multi-implication scenarios, we train the task of implication number prediction together with the recall task in a multi-task learning setting, since these two tasks are highly complementary. In rank step, we propose a keywords attentive mechanism to focus on domain-specific information such as procedure sites and procedure types. Finally, a fusion block merges the results of the recall and the rank model. Detailed experimental analysis shows our proposed framework has a remarkable improvement on both performance and efficiency. AVAILABILITY: The source code will be available at https://github.com/sxthunder/CMTN upon publication.},
author = {Liang, Ming and Xue, Kui and Ye, Qi and Ruan, Tong},
doi = {10.1093/bioinformatics/btab381},
editor = {Valencia, Alfonso},
issn = {1367-4803},
journal = {Bioinformatics},
language = {eng},
month = {oct},
number = {20},
pages = {3610--3617},
pmid = {34037691},
title = {{A combined recall and rank framework with online negative sampling for Chinese procedure terminology normalization}},
url = {https://academic.oup.com/bioinformatics/article/37/20/3610/6284956},
volume = {37},
year = {2021}
}
@article{Nath2021,
abstract = {BACKGROUND: Word vectors or word embeddings are n-dimensional representations of words and form the backbone of Natural Language Processing of textual data. This research experiments with algorithms that augment word vectors with lexical constraints that are popular in NLP research and clinical domain constraints derived from the Unified Medical Language System (UMLS). It also compares the performance of the augmented vectors with Bio + Clinical BERT vectors which have been trained and fine-tuned on clinical datasets. METHODS: Word2vec vectors are generated for words in a publicly available de-identified Electronic Health Records (EHR) dataset and augmented by ontologies using three algorithms that have fundamentally different approaches to vector augmentation. The augmented vectors are then evaluated alongside publicly available Bio + Clinical BERT on their correlation with human-annotated lists using Spearman's correlation coefficient. They are also evaluated on the downstream task of Named Entity Recognition (NER). Quantitative and empirical evaluations are used to highlight the strengths and weaknesses of the different approaches. RESULTS: The counter-fitted word2vec vectors augmented with information from the UMLS ontology produced the best correlation overall with human-annotated evaluation lists (Spearman's correlation of 0.733 with mini mayo-doctors' annotation) while Bio + Clinical BERT produces the best results in the NER task (F1 of 0.87 and 0.811 on the i2b2 2010 and i2b2 2012 datasets respectively) in our experiments. CONCLUSION: Clinically adapted word2vec vectors successfully encapsulate concepts of lexical and clinical synonymy and antonymy and to a smaller extent, hyponymy and hypernymy. Bio + Clinical BERT vectors perform better at NER and avoid out-of-vocabulary words.},
author = {Nath, Namrata and Lee, Sang-Heon and McDonnell, Mark D and Lee, Ivan},
doi = {10.1016/j.compbiomed.2021.104433},
issn = {00104825},
journal = {Computers in Biology and Medicine},
keywords = {Algorithms,Electronic Health Records,Humans,Natural Language Processing,Unified Medical Language System},
language = {eng},
month = {jul},
pages = {104433},
pmid = {34004575},
title = {{The quest for better clinical word vectors: Ontology based and lexical vector augmentation versus clinical contextual embeddings}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0010482521002274},
volume = {134},
year = {2021}
}
@article{Yuan2020,
abstract = {OBJECTIVE: This study aims at realizing unsupervised term discovery in Chinese electronic health records (EHRs) by using the word segmentation technique. The existing supervised algorithms do not perform satisfactorily in the case of EHRs, as annotated medical data are scarce. We propose an unsupervised segmentation method (GTS) based on the graph partition principle, whose multi-granular segmentation capability can help realize efficient term discovery. METHODS: A sentence is converted to an undirected graph, with the edge weights based on n-gram statistics, and ratio cut is used to split the sentence into words. The graph partition is solved efficiently via dynamic programming, and multi-granularity is realized by setting different partition numbers. A BERT-based discriminator is trained using generated samples to verify the correctness of the word boundaries. The words that are not recorded in existing dictionaries are retained as potential medical terms. RESULTS: We compared the GTS approach with mature segmentation systems for both word segmentation and term discovery. MD students manually segmented Chinese EHRs at fine and coarse granularity levels and reviewed the term discovery results. The proposed unsupervised method outperformed all the competing algorithms in the word segmentation task. In term discovery, GTS outperformed the best baseline by 17 percentage points (a 47% relative percentage of increment) on F1-score. CONCLUSION: In the absence of annotated training data, the graph partition technique can effectively use the corpus statistics and even expert knowledge to realize unsupervised word segmentation of EHRs. Multi-granular segmentation can be used to provide potential medical terms of various lengths with high accuracy.},
author = {Yuan, Zheng and Liu, Yuanhao and Yin, Qiuyang and Li, Boyao and Feng, Xiaobin and Zhang, Guoming and Yu, Sheng},
doi = {10.1016/j.jbi.2020.103542},
issn = {15320464},
journal = {Journal of Biomedical Informatics},
keywords = {Algorithms,China,Electronic Health Records,Humans,Language},
language = {eng},
month = {oct},
pages = {103542},
pmid = {32853795},
title = {{Unsupervised multi-granular Chinese word segmentation and term discovery via graph partition}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1532046420301702},
volume = {110},
year = {2020}
}
@article{Li2019,
abstract = {BACKGROUND: The bidirectional encoder representations from transformers (BERT) model has achieved great success in many natural language processing (NLP) tasks, such as named entity recognition and question answering. However, little prior work has explored this model to be used for an important task in the biomedical and clinical domains, namely entity normalization. OBJECTIVE: We aim to investigate the effectiveness of BERT-based models for biomedical or clinical entity normalization. In addition, our second objective is to investigate whether the domains of training data influence the performances of BERT-based models as well as the degree of influence. METHODS: Our data was comprised of 1.5 million unlabeled electronic health record (EHR) notes. We first fine-tuned BioBERT on this large collection of unlabeled EHR notes. This generated our BERT-based model trained using 1.5 million electronic health record notes (EhrBERT). We then further fine-tuned EhrBERT, BioBERT, and BERT on three annotated corpora for biomedical and clinical entity normalization: the Medication, Indication, and Adverse Drug Events (MADE) 1.0 corpus, the National Center for Biotechnology Information (NCBI) disease corpus, and the Chemical-Disease Relations (CDR) corpus. We compared our models with two state-of-the-art normalization systems, namely MetaMap and disease name normalization (DNorm). RESULTS: EhrBERT achieved 40.95% F1 in the MADE 1.0 corpus for mapping named entities to the Medical Dictionary for Regulatory Activities and the Systematized Nomenclature of Medicine-Clinical Terms (SNOMED-CT), which have about 380,000 terms. In this corpus, EhrBERT outperformed MetaMap by 2.36% in F1. For the NCBI disease corpus and CDR corpus, EhrBERT also outperformed DNorm by improving the F1 scores from 88.37% and 89.92% to 90.35% and 93.82%, respectively. Compared with BioBERT and BERT, EhrBERT outperformed them on the MADE 1.0 corpus and the CDR corpus. CONCLUSIONS: Our work shows that BERT-based models have achieved state-of-the-art performance for biomedical and clinical entity normalization. BERT-based models can be readily fine-tuned to normalize any kind of named entities.},
author = {Li, Fei and Jin, Yonghao and Liu, Weisong and Rawat, Bhanu Pratap Singh and Cai, Pengshan and Yu, Hong},
doi = {10.2196/14830},
issn = {2291-9694},
journal = {JMIR Medical Informatics},
language = {eng},
month = {sep},
number = {3},
pages = {e14830},
pmid = {31516126},
title = {{Fine-Tuning Bidirectional Encoder Representations From Transformers (BERT)–Based Models on Large-Scale Electronic Health Record Notes: An Empirical Study}},
url = {http://medinform.jmir.org/2019/3/e14830/},
volume = {7},
year = {2019}
}
@article{Shin2021,
abstract = {BACKGROUND: In the case of Korean institutions and enterprises that collect nonstandardized and nonunified formats of electronic medical examination results from multiple medical institutions, a group of experienced nurses who can understand the results and related contexts initially classified the reports manually. The classification guidelines were established by years of workers' clinical experiences and there were attempts to automate the classification work. However, there have been problems in which rule-based algorithms or human labor-intensive efforts can be time-consuming or limited owing to high potential errors. We investigated natural language processing (NLP) architectures and proposed ensemble models to create automated classifiers. OBJECTIVE: This study aimed to develop practical deep learning models with electronic medical records from 284 health care institutions and open-source corpus data sets for automatically classifying 3 thyroid conditions: healthy, caution required, and critical. The primary goal is to increase the overall accuracy of the classification, yet there are practical and industrial needs to correctly predict healthy (negative) thyroid condition data, which are mostly medical examination results, and minimize false-negative rates under the prediction of healthy thyroid conditions. METHODS: The data sets included thyroid and comprehensive medical examination reports. The textual data are not only documented in fully complete sentences but also written in lists of words or phrases. Therefore, we propose static and contextualized ensemble NLP network (SCENT) systems to successfully reflect static and contextual information and handle incomplete sentences. We prepared each convolution neural network (CNN)-, long short-term memory (LSTM)-, and efficiently learning an encoder that classifies token replacements accurately (ELECTRA)-based ensemble model by training or fine-tuning them multiple times. Through comprehensive experiments, we propose 2 versions of ensemble models, SCENT-v1 and SCENT-v2, with the single-architecture-based CNN, LSTM, and ELECTRA ensemble models for the best classification performance and practical use, respectively. SCENT-v1 is an ensemble of CNN and ELECTRA ensemble models, and SCENT-v2 is a hierarchical ensemble of CNN, LSTM, and ELECTRA ensemble models. SCENT-v2 first classifies the 3 labels using an ELECTRA ensemble model and then reclassifies them using an ensemble model of CNN and LSTM if the ELECTRA ensemble model predicted them as "healthy" labels. RESULTS: SCENT-v1 outperformed all the suggested models, with the highest F1 score (92.56%). SCENT-v2 had the second-highest recall value (94.44%) and the fewest misclassifications for caution-required thyroid condition while maintaining 0 classification error for the critical thyroid condition under the prediction of the healthy thyroid condition. CONCLUSIONS: The proposed SCENT demonstrates good classification performance despite the unique characteristics of the Korean language and problems of data lack and imbalance, especially for the extremely low amount of critical condition data. The result of SCENT-v1 indicates that different perspectives of static and contextual input token representations can enhance classification performance. SCENT-v2 has a strong impact on the prediction of healthy thyroid conditions.},
author = {Shin, Dongyup and Kam, Hye Jin and Jeon, Min-Seok and Kim, Ha Young},
doi = {10.2196/30223},
issn = {2291-9694},
journal = {JMIR Medical Informatics},
language = {eng},
month = {sep},
number = {9},
pages = {e30223},
pmid = {34546183},
title = {{Automatic Classification of Thyroid Findings Using Static and Contextualized Ensemble Natural Language Processing Systems: Development Study}},
url = {https://medinform.jmir.org/2021/9/e30223},
volume = {9},
year = {2021}
}
@article{Soni2020,
abstract = {We apply deep learning-based language models to the task of patient cohort retrieval (CR) with the aim to assess their efficacy. The task ofCR requires the extraction of relevant documents from the electronic health records (EHRs) on the basis of a given query. Given the recent advancements in the field of document retrieval, we map the task of CR to a document retrieval task and apply various deep neural models implemented for the general domain tasks. In this paper, we propose a framework for retrieving patient cohorts using neural language models without the need of explicit feature engineering and domain expertise. We find that a majority of our models outperform the BM25 baseline method on various evaluation metrics.},
author = {Soni, Sarvesh and Roberts, Kirk},
issn = {1942-597X},
journal = {AMIA ... Annual Symposium proceedings. AMIA Symposium},
keywords = {Cohort Studies,Deep Learning,Electronic Health Records,Humans,Information Storage and Retrieval,Language,Natural Language Processing},
language = {eng},
pages = {1150--1159},
pmid = {33936491},
title = {{Patient Cohort Retrieval using Transformer Language Models.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/33936491 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC8075458},
volume = {2020},
year = {2020}
}
@article{Zhan2021,
abstract = {BACKGROUND: Family history information, including information on family members, side of the family of family members, living status of family members, and observations of family members, plays an important role in disease diagnosis and treatment. Family member information extraction aims to extract family history information from semistructured/unstructured text in electronic health records (EHRs), which is a challenging task regarding named entity recognition (NER) and relation extraction (RE), where named entities refer to family members, living status, and observations, and relations refer to relations between family members and living status, and relations between family members and observations. OBJECTIVE: This study aimed to introduce the system we developed for the 2019 n2c2/OHNLP track on family history extraction, which can jointly extract entities and relations about family history information from clinical text. METHODS: We proposed a novel graph-based model with biaffine attention for family history extraction from clinical text. In this model, we first designed a graph to represent family history information, that is, representing NER and RE regarding family history in a unified way, and then introduced a biaffine attention mechanism to extract family history information in clinical text. Convolution neural network (CNN)-Bidirectional Long Short Term Memory network (BiLSTM) and Bidirectional Encoder Representation from Transformers (BERT) were used to encode the input sentence, and a biaffine classifier was used to extract family history information. In addition, we developed a postprocessing module to adjust the results. A system based on the proposed method was developed for the 2019 n2c2/OHNLP shared task track on family history information extraction. RESULTS: Our system ranked first in the challenge, and the F1 scores of the best system on the NER subtask and RE subtask were 0.8745 and 0.6810, respectively. After the challenge, we further fine tuned the parameters and improved the F1 scores of the two subtasks to 0.8823 and 0.7048, respectively. CONCLUSIONS: The experimental results showed that the system based on the proposed method can extract family history information from clinical text effectively.},
author = {Zhan, Kecheng and Peng, Weihua and Xiong, Ying and Fu, Huhao and Chen, Qingcai and Wang, Xiaolong and Tang, Buzhou},
doi = {10.2196/23587},
issn = {2291-9694},
journal = {JMIR Medical Informatics},
language = {eng},
month = {apr},
number = {4},
pages = {e23587},
pmid = {33881405},
title = {{Novel Graph-Based Model With Biaffine Attention for Family History Extraction From Clinical Text: Modeling Study}},
url = {https://medinform.jmir.org/2021/4/e23587},
volume = {9},
year = {2021}
}
@article{Pan2021,
abstract = {BACKGROUND: Electronic medical records (EMRs) are usually stored in relational databases that require SQL queries to retrieve information of interest. Effectively completing such queries can be a challenging task for medical experts due to the barriers in expertise. Existing text-to-SQL generation studies have not been fully embraced in the medical domain. OBJECTIVE: The objective of this study was to propose a neural generation model that can jointly consider the characteristics of medical text and the SQL structure to automatically transform medical texts to SQL queries for EMRs. METHODS: We proposed a medical text-to-SQL model (MedTS), which employed a pretrained Bidirectional Encoder Representations From Transformers model as the encoder and leveraged a grammar-based long short-term memory network as the decoder to predict the intermediate representation that can easily be transformed into the final SQL query. We adopted the syntax tree as the intermediate representation rather than directly regarding the SQL query as an ordinary word sequence, which is more in line with the tree-structure nature of SQL and can also effectively reduce the search space during generation. Experiments were conducted on the MIMICSQL dataset, and 5 competitor methods were compared. RESULTS: Experimental results demonstrated that MedTS achieved the accuracy of 0.784 and 0.899 on the test set in terms of logic form and execution, respectively, which significantly outperformed the existing state-of-the-art methods. Further analyses proved that the performance on each component of the generated SQL was relatively balanced and offered substantial improvements. CONCLUSIONS: The proposed MedTS was effective and robust for improving the performance of medical text-to-SQL generation, indicating strong potential to be applied in the real medical scenario.},
author = {Pan, Youcheng and Wang, Chenghao and Hu, Baotian and Xiang, Yang and Wang, Xiaolong and Chen, Qingcai and Chen, Junjie and Du, Jingcheng},
doi = {10.2196/32698},
issn = {2291-9694},
journal = {JMIR Medical Informatics},
language = {eng},
month = {dec},
number = {12},
pages = {e32698},
pmid = {34889749},
title = {{A BERT-Based Generation Model to Transform Medical Texts to SQL Queries for Electronic Medical Records: Model Development and Validation}},
url = {https://medinform.jmir.org/2021/12/e32698},
volume = {9},
year = {2021}
}
@inproceedings{10.1145/3459930.3469560,
abstract = {The rapid adoption of electronic health records (EHRs) systems has made clinical data
available in electronic format for research and for many downstream applications.
Electronic screening of potentially eligible patients using these clinical databases
for clinical trials is a critical need to improve trial recruitment efficiency. Nevertheless,
manually translating free-text eligibility criteria into database queries is labor
intensive and inefficient. To facilitate automated screening, free-text eligibility
criteria must be structured and coded into a computable format using controlled vocabularies.
Named entity recognition (NER) is thus an important first step. In this study, we
evaluate 4 state-of-the-art transformer-based NER models on two publicly available
annotated corpora of eligibility criteria released by Columbia University (i.e., the
Chia data) and Facebook Research (i.e.the FRD data). Four transformer-based models
(i.e., BERT, ALBERT, RoBERTa, and ELECTRA) pretrained with general English domain
corpora vs. those pretrained with PubMed citations, clinical notes from the MIMIC-III
dataset and eligibility criteria extracted from all the clinical trials on ClinicalTrials.gov
were compared. Experimental results show that RoBERTa pretrained with MIMIC-III clinical
notes and eligibility criteria yielded the highest strict and relaxed F-scores in
both the Chia data (i.e., 0.658/0.798) and the FRD data (i.e., 0.785/0.916). With
promising NER results, further investigations on building a reliable natural language
processing (NLP)-assisted pipeline for automated electronic screening are needed.},
address = {New York, NY, USA},
author = {Tian, Shubo and Erdengasileng, Arslan and Yang, Xi and Guo, Yi and Wu, Yonghui and Zhang, Jinfeng and Bian, Jiang and He, Zhe},
booktitle = {Proceedings of the 12th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics},
doi = {10.1145/3459930.3469560},
isbn = {9781450384506},
keywords = {clinical trial,eligibility criteria parsing,named entity recognition,transformer-based model},
publisher = {Association for Computing Machinery},
series = {BCB '21},
title = {{Transformer-Based Named Entity Recognition for Parsing Clinical Trial Eligibility Criteria}},
url = {https://doi.org/10.1145/3459930.3469560},
year = {2021}
}
@article{Alimova2020,
abstract = {Relation extraction aims to discover relational facts about entity mentions from plain texts. In this work, we focus on clinical relation extraction; namely, given a medical record with mentions of drugs and their attributes, we identify relations between these entities. We propose a machine learning model with a novel set of knowledge-based and BioSentVec embedding features. We systematically investigate the impact of these features with standard distance- and word-based features, conducting experiments on two benchmark datasets of clinical texts from MADE 2018 and n2c2 2018 shared tasks. For comparison with the feature-based model, we utilize state-of-the-art models and three BERT-based models, including BioBERT and Clinical BERT. Our results demonstrate that distance and word features provide significant benefits to the classifier. Knowledge-based features improve classification results only for particular types of relations. The sentence embedding feature provides the largest improvement in results, among other explored features on the MADE corpus. The classifier obtains state-of-the-art performance in clinical relation extraction with F-measure of 92.6%, improving F-measure by 3.5% on the MADE corpus.},
author = {Alimova, Ilseyar and Tutubalina, Elena},
doi = {10.1016/j.jbi.2020.103382},
issn = {15320464},
journal = {Journal of Biomedical Informatics},
keywords = {Knowledge Bases,Language,Machine Learning,Natural Language Processing},
language = {eng},
month = {mar},
pages = {103382},
pmid = {32028051},
title = {{Multiple features for clinical relation extraction: A machine learning approach}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1532046420300095},
volume = {103},
year = {2020}
}
@article{Khambete2021,
abstract = {Deep learning models in healthcare may fail to generalize on data from unseen corpora. Additionally, no quantitative metric exists to tell how existing models will perform on new data. Previous studies demonstrated that NLP models of medical notes generalize variably between institutions, but ignored other levels of healthcare organization. We measured SciBERT diagnosis sentiment classifier generalizability between medical specialties using EHR sentences from MIMIC-III. Models trained on one specialty performed better on internal test sets than mixed or external test sets (mean AUCs 0.92, 0.87, and 0.83, respectively; p = 0.016). When models are trained on more specialties, they have better test performances (p < 1e-4). Model performance on new corpora is directly correlated to the similarity between train and test sentence content (p < 1e-4). Future studies should assess additional axes of generalization to ensure deep learning models fulfil their intended purpose across institutions, specialties, and practices.},
author = {Khambete, Mihir P and Su, William and Garcia, Juan C and Badgeley, Marcus A},
issn = {2153-4063},
journal = {AMIA Joint Summits on Translational Science proceedings. AMIA Joint Summits on Translational Science},
keywords = {Deep Learning,Humans,Language,Medicine,Semantics},
language = {eng},
pages = {345--354},
pmid = {34457149},
title = {{Quantification of BERT Diagnosis Generalizability Across Medical Specialties Using Semantic Dataset Distance.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/34457149 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC8378651},
volume = {2021},
year = {2021}
}
@article{Lin2021,
abstract = {Medical records scoring is important in a health care system. Artificial intelligence (AI) with projection word embeddings has been validated in its performance disease coding tasks, which maintain the vocabulary diversity of open internet databases and the medical terminology understanding of electronic health records (EHRs). We considered that an AI-enhanced system might be also applied to automatically score medical records. This study aimed to develop a series of deep learning models (DLMs) and validated their performance in medical records scoring task. We also analyzed the practical value of the best model. We used the admission medical records from the Tri-Services General Hospital during January 2016 to May 2020, which were scored by our visiting staffs with different levels from different departments. The medical records were scored ranged 0 to 10. All samples were divided into a training set (n = 74,959) and testing set (n = 152,730) based on time, which were used to train and validate the DLMs, respectively. The mean absolute error (MAE) was used to evaluate each DLM performance. In original AI medical record scoring, the predicted score by BERT architecture is closer to the actual reviewer score than the projection word embedding and LSTM architecture. The original MAE is 0.84 ± 0.27 using the BERT model, and the MAE is 1.00 ± 0.32 using the LSTM model. Linear mixed model can be used to improve the model performance, and the adjusted predicted score was closer compared to the original score. However, the project word embedding with the LSTM model (0.66 ± 0.39) provided better performance compared to BERT (0.70 ± 0.33) after linear mixed model enhancement (p < 0.001). In addition to comparing different architectures to score the medical records, this study further uses a mixed linear model to successfully adjust the AI medical record score to make it closer to the actual physician's score.},
author = {Lin, Chin and Lee, Yung-Tsai and Wu, Feng-Jen and Lin, Shing-An and Hsu, Chia-Jung and Lee, Chia-Cheng and Tsai, Dung-Jang and Fang, Wen-Hui},
doi = {10.3390/healthcare9101298},
issn = {2227-9032},
journal = {Healthcare},
language = {eng},
month = {sep},
number = {10},
pages = {1298},
pmid = {34682978},
title = {{The Application of Projection Word Embeddings on Medical Records Scoring System}},
url = {https://www.mdpi.com/2227-9032/9/10/1298},
volume = {9},
year = {2021}
}
@article{Luo2021,
abstract = {BACKGROUND AND OBJECTIVE: Chronic cough (CC) affects approximately 10% of adults. Many disease states are associated with chronic cough, such as asthma, upper airway cough syndrome, bronchitis, and gastroesophageal reflux disease. The lack of an ICD code specific for chronic cough makes it challenging to identify such patients from electronic health records (EHRs). For clinical and research purposes, computational methods using EHR data are urgently needed to identify chronic cough cases. This research aims to investigate the data representations and deep learning algorithms for chronic cough prediction. METHODS: Utilizing real-world EHR data from a large academic healthcare system from October 2005 to September 2015, we investigated Natural Language Representation of the EHR data and systematically evaluated deep learning and traditional machine learning models to predict chronic cough patients. We built these machine learning models using structured data (medication and diagnosis) and unstructured data (clinical notes). RESULTS: The sensitivity and specificity of a transformer-based deep learning algorithm, specifically BERT with attention model, was 0.856 and 0.866, respectively, using structured data (medication and diagnosis). Sensitivity and specificity improved to 0.952 and 0.930 when we combined structured data with symptoms extracted from clinical notes. We further found that the attention mechanism of deep learning models can be used to extract important features that drive the prediction decisions. Compared with our previously published rule-based algorithm, the deep learning algorithm can identify more chronic cough patients with structured data. CONCLUSIONS: By applying deep learning models, chronic cough patients can be reliably identified for prospective or retrospective research through medication and diagnosis data, widely available in EHR and electronic claims data, thus improving the generalizability of the patient identification algorithm. Deep learning models can identify chronic cough patients with even higher sensitivity and specificity when structured and unstructured EHR data are utilized. We anticipate language-based data representation and deep learning models developed in this research could also be productively used for other disease prediction and case identification.},
author = {Luo, Xiao and Gandhi, Priyanka and Zhang, Zuoyi and Shao, Wei and Han, Zhi and Chandrasekaran, Vasu and Turzhitsky, Vladimir and Bali, Vishal and Roberts, Anna R and Metzger, Megan and Baker, Jarod and {La Rosa}, Carmen and Weaver, Jessica and Dexter, Paul and Huang, Kun},
doi = {10.1016/j.cmpb.2021.106395},
issn = {01692607},
journal = {Computer Methods and Programs in Biomedicine},
keywords = {Adult,Algorithms,Cough,Deep Learning,Electronic Health Records,Humans,Machine Learning,Prospective Studies,Retrospective Studies,diagnosis},
language = {eng},
month = {oct},
pages = {106395},
pmid = {34525412},
title = {{Applying interpretable deep learning models to identify chronic cough patients using EHR data}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0169260721004697},
volume = {210},
year = {2021}
}
@article{Wu2021,
abstract = {BACKGROUND: With the increasing variety of drugs, the incidence of adverse drug events (ADEs) is increasing year by year. Massive numbers of ADEs are recorded in electronic medical records and adverse drug reaction (ADR) reports, which are important sources of potential ADR information. Meanwhile, it is essential to make latent ADR information automatically available for better postmarketing drug safety reevaluation and pharmacovigilance. OBJECTIVE: This study describes how to identify ADR-related information from Chinese ADE reports. METHODS: Our study established an efficient automated tool, named BBC-Radical. BBC-Radical is a model that consists of 3 components: Bidirectional Encoder Representations from Transformers (BERT), bidirectional long short-term memory (bi-LSTM), and conditional random field (CRF). The model identifies ADR-related information from Chinese ADR reports. Token features and radical features of Chinese characters were used to represent the common meaning of a group of words. BERT and Bi-LSTM-CRF were novel models that combined these features to conduct named entity recognition (NER) tasks in the free-text section of 24,890 ADR reports from the Jiangsu Province Adverse Drug Reaction Monitoring Center from 2010 to 2016. Moreover, the man-machine comparison experiment on the ADE records from Drum Tower Hospital was designed to compare the NER performance between the BBC-Radical model and a manual method. RESULTS: The NER model achieved relatively high performance, with a precision of 96.4%, recall of 96.0%, and F1 score of 96.2%. This indicates that the performance of the BBC-Radical model (precision 87.2%, recall 85.7%, and F1 score 86.4%) is much better than that of the manual method (precision 86.1%, recall 73.8%, and F1 score 79.5%) in the recognition task of each kind of entity. CONCLUSIONS: The proposed model was competitive in extracting ADR-related information from ADE reports, and the results suggest that the application of our method to extract ADR-related information is of great significance in improving the quality of ADR reports and postmarketing drug safety evaluation.},
author = {Wu, Hong and Ji, Jiatong and Tian, Haimei and Chen, Yao and Ge, Weihong and Zhang, Haixia and Yu, Feng and Zou, Jianjun and Nakamura, Mitsuhiro and Liao, Jun},
doi = {10.2196/26407},
issn = {2291-9694},
journal = {JMIR Medical Informatics},
language = {eng},
month = {dec},
number = {12},
pages = {e26407},
pmid = {34855616},
title = {{Chinese-Named Entity Recognition From Adverse Drug Event Records: Radical Embedding-Combined Dynamic Embedding–Based BERT in a Bidirectional Long Short-term Conditional Random Field (Bi-LSTM-CRF) Model}},
url = {https://medinform.jmir.org/2021/12/e26407},
volume = {9},
year = {2021}
}
@article{Searle2021,
abstract = {The current mode of use of Electronic Health Records (EHR) elicits text redundancy. Clinicians often populate new documents by duplicating existing notes, then updating accordingly. Data duplication can lead to propagation of errors, inconsistencies and misreporting of care. Therefore, measures to quantify information redundancy play an essential role in evaluating innovations that operate on clinical narratives. This work is a quantitative examination of information redundancy in EHR notes. We present and evaluate two methods to measure redundancy: an information-theoretic approach and a lexicosyntactic and semantic model. Our first measure trains large Transformer-based language models using clinical text from a large openly available US-based ICU dataset and a large multi-site UK based Hospital. By comparing the information-theoretic efficient encoding of clinical text against open-domain corpora, we find that clinical text is ∼1.5× to ∼3× less efficient than open-domain corpora at conveying information. Our second measure, evaluates automated summarisation metrics Rouge and BERTScore to evaluate successive note pairs demonstrating lexicosyntactic and semantic redundancy, with averages from ∼43 to ∼65%.},
author = {Searle, Thomas and Ibrahim, Zina and Teo, James and Dobson, Richard},
doi = {10.1016/j.jbi.2021.103938},
issn = {1532-0480},
journal = {Journal of biomedical informatics},
keywords = {Deep transfer learning for language modelling of c,Natural language processing methods to estimate re},
language = {eng},
month = {dec},
pages = {103938},
pmid = {34695581},
title = {{Estimating redundancy in clinical text.}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1532046421002677 http://www.ncbi.nlm.nih.gov/pubmed/34695581},
volume = {124},
year = {2021}
}
@article{Zhang2020,
abstract = {BACKGROUND: Medical entity recognition is a key technology that supports the development of smart medicine. Existing methods on English medical entity recognition have undergone great development, but their progress in the Chinese language has been slow. Because of limitations due to the complexity of the Chinese language and annotated corpora, these methods are based on simple neural networks, which cannot effectively extract the deep semantic representations of electronic medical records (EMRs) and be used on the scarce medical corpora. We thus developed a new Chinese EMR (CEMR) dataset with six types of entities and proposed a multi-level representation learning model based on Bidirectional Encoder Representation from Transformers (BERT) for Chinese medical entity recognition. OBJECTIVE: This study aimed to improve the performance of the language model by having it learn multi-level representation and recognize Chinese medical entities. METHODS: In this paper, the pretraining language representation model was investigated; utilizing information not only from the final layer but from intermediate layers was found to affect the performance of the Chinese medical entity recognition task. Therefore, we proposed a multi-level representation learning model for entity recognition in Chinese EMRs. Specifically, we first used the BERT language model to extract semantic representations. Then, the multi-head attention mechanism was leveraged to automatically extract deeper semantic information from each layer. Finally, semantic representations from multi-level representation extraction were utilized as the final semantic context embedding for each token and we used softmax to predict the entity tags. RESULTS: The best F1 score reached by the experiment was 82.11% when using the CEMR dataset, and the F1 score when using the CCKS (China Conference on Knowledge Graph and Semantic Computing) 2018 benchmark dataset further increased to 83.18%. Various comparative experiments showed that our proposed method outperforms methods from previous work and performs as a new state-of-the-art method. CONCLUSIONS: The multi-level representation learning model is proposed as a method to perform the Chinese EMRs entity recognition task. Experiments on two clinical datasets demonstrate the usefulness of using the multi-head attention mechanism to extract multi-level representation as part of the language model.},
author = {Zhang, Zhichang and Zhu, Lin and Yu, Peilin},
doi = {10.2196/17637},
issn = {2291-9694},
journal = {JMIR Medical Informatics},
language = {eng},
month = {may},
number = {5},
pages = {e17637},
pmid = {32364514},
title = {{Multi-Level Representation Learning for Chinese Medical Entity Recognition: Model Development and Validation}},
url = {https://medinform.jmir.org/2020/5/e17637},
volume = {8},
year = {2020}
}
@article{Christopoulou2020,
abstract = {OBJECTIVE: Identification of drugs, associated medication entities, and interactions among them are crucial to prevent unwanted effects of drug therapy, known as adverse drug events. This article describes our participation to the n2c2 shared-task in extracting relations between medication-related entities in electronic health records. MATERIALS AND METHODS: We proposed an ensemble approach for relation extraction and classification between drugs and medication-related entities. We incorporated state-of-the-art named-entity recognition (NER) models based on bidirectional long short-term memory (BiLSTM) networks and conditional random fields (CRF) for end-to-end extraction. We additionally developed separate models for intra- and inter-sentence relation extraction and combined them using an ensemble method. The intra-sentence models rely on bidirectional long short-term memory networks and attention mechanisms and are able to capture dependencies between multiple related pairs in the same sentence. For the inter-sentence relations, we adopted a neural architecture that utilizes the Transformer network to improve performance in longer sequences. RESULTS: Our team ranked third with a micro-averaged F1 score of 94.72% and 87.65% for relation and end-to-end relation extraction, respectively (Tracks 2 and 3). Our ensemble effectively takes advantages from our proposed models. Analysis of the reported results indicated that our proposed approach is more generalizable than the top-performing system, which employs additional training data- and corpus-driven processing techniques. CONCLUSIONS: We proposed a relation extraction system to identify relations between drugs and medication-related entities. The proposed approach is independent of external syntactic tools. Analysis showed that by using latent Drug-Drug interactions we were able to significantly improve the performance of non-Drug-Drug pairs in EHRs.},
author = {Christopoulou, Fenia and Tran, Thy Thy and Sahu, Sunil Kumar and Miwa, Makoto and Ananiadou, Sophia},
doi = {10.1093/jamia/ocz101},
issn = {1527-974X},
journal = {Journal of the American Medical Informatics Association},
keywords = {Computer,Deep Learning,Drug Interactions,Drug-Related Side Effects and Adverse Reactions,Electronic Health Records,Humans,Information Storage and Retrieval,Natural Language Processing,Neural Networks,methods},
language = {eng},
month = {jan},
number = {1},
pages = {39--46},
pmid = {31390003},
title = {{Adverse drug events and medication relation extraction in electronic health records with ensemble deep learning methods}},
url = {https://academic.oup.com/jamia/article/27/1/39/5544735},
volume = {27},
year = {2020}
}
@article{Naderi2021,
abstract = {The health and life science domains are well known for their wealth of named entities found in large free text corpora, such as scientific literature and electronic health records. To unlock the value of such corpora, named entity recognition (NER) methods are proposed. Inspired by the success of transformer-based pretrained models for NER, we assess how individual and ensemble of deep masked language models perform across corpora of different health and life science domains—biology, chemistry, and medicine—available in different languages—English and French. Individual deep masked language models, pretrained on external corpora, are fined-tuned on task-specific domain and language corpora and ensembled using classical majority voting strategies. Experiments show statistically significant improvement of the ensemble models over an individual BERT-based baseline model, with an overall best performance of 77% macro F1-score. We further perform a detailed analysis of the ensemble results and show how their effectiveness changes according to entity properties, such as length, corpus frequency, and annotation consistency. The results suggest that the ensembles of deep masked language models are an effective strategy for tackling NER across corpora from the health and life science domains.},
author = {Naderi, Nona and Knafou, Julien and Copara, Jenny and Ruch, Patrick and Teodoro, Douglas},
doi = {10.3389/frma.2021.689803},
issn = {2504-0537},
journal = {Frontiers in Research Metrics and Analytics},
language = {eng},
month = {nov},
pages = {689803},
pmid = {34870074},
title = {{Ensemble of Deep Masked Language Models for Effective Named Entity Recognition in Health and Life Science Corpora}},
url = {https://www.frontiersin.org/articles/10.3389/frma.2021.689803/full},
volume = {6},
year = {2021}
}
@article{Mitra2020,
abstract = {A bleeding event is a common adverse drug reaction amongst patients on anticoagulation and factors critically into a clinician's decision to prescribe or continue anticoagulation for atrial fibrillation. However, bleeding events are not uniformly captured in the administrative data of electronic health records (EHR). As manual review is prohibitively expensive, we investigate the effectiveness of various natural language processing (NLP) methods for automatic extraction of bleeding events. Using our expert-annotated 1,079 de-identified EHR notes, we evaluated state-of-the-art NLP models such as biLSTM-CRF with language modeling, and different BERT variants for six entity types. On our dataset, the biLSTM-CRF surpassed other models resulting in a macro F1-score of 0.75 whereas the performance difference is negligible for sentence and document-level predictions with the best macro F1-scores of 0.84 and 0.96, respectively. Our error analyses suggest that the models' incorrect predictions can be attributed to variability in entity spans, memorization, and missing negation signals.},
author = {Mitra, Avijit and Rawat, Bhanu Pratap Singh and McManus, David and Kapoor, Alok and Yu, Hong},
issn = {1942-597X},
journal = {AMIA ... Annual Symposium proceedings. AMIA Symposium},
keywords = {Drug-Related Side Effects and Adverse Reactions,Electronic Health Records,Hemorrhage,Humans,Language,Natural Language Processing,diagnosis},
language = {eng},
pages = {860--869},
pmid = {33936461},
title = {{Bleeding Entity Recognition in Electronic Health Records: A Comprehensive Analysis of End-to-End Systems.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/33936461 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC8075442},
volume = {2020},
year = {2020}
}
@article{Xiong2020,
abstract = {BACKGROUND: With the popularity of electronic health records (EHRs), the quality of health care has been improved. However, there are also some problems caused by EHRs, such as the growing use of copy-and-paste and templates, resulting in EHRs of low quality in content. In order to minimize data redundancy in different documents, Harvard Medical School and Mayo Clinic organized a national natural language processing (NLP) clinical challenge (n2c2) on clinical semantic textual similarity (ClinicalSTS) in 2019. The task of this challenge is to compute the semantic similarity among clinical text snippets. OBJECTIVE: In this study, we aim to investigate novel methods to model ClinicalSTS and analyze the results. METHODS: We propose a semantically enhanced text matching model for the 2019 n2c2/Open Health NLP (OHNLP) challenge on ClinicalSTS. The model includes 3 representation modules to encode clinical text snippet pairs at different levels: (1) character-level representation module based on convolutional neural network (CNN) to tackle the out-of-vocabulary problem in NLP; (2) sentence-level representation module that adopts a pretrained language model bidirectional encoder representation from transformers (BERT) to encode clinical text snippet pairs; and (3) entity-level representation module to model clinical entity information in clinical text snippets. In the case of entity-level representation, we compare 2 methods. One encodes entities by the entity-type label sequence corresponding to text snippet (called entity I), whereas the other encodes entities by their representation in MeSH, a knowledge graph in the medical domain (called entity II). RESULTS: We conduct experiments on the ClinicalSTS corpus of the 2019 n2c2/OHNLP challenge for model performance evaluation. The model only using BERT for text snippet pair encoding achieved a Pearson correlation coefficient (PCC) of 0.848. When character-level representation and entity-level representation are individually added into our model, the PCC increased to 0.857 and 0.854 (entity I)/0.859 (entity II), respectively. When both character-level representation and entity-level representation are added into our model, the PCC further increased to 0.861 (entity I) and 0.868 (entity II). CONCLUSIONS: Experimental results show that both character-level information and entity-level information can effectively enhance the BERT-based STS model.},
author = {Xiong, Ying and Chen, Shuai and Chen, Qingcai and Yan, Jun and Tang, Buzhou},
doi = {10.2196/23357},
issn = {2291-9694},
journal = {JMIR Medical Informatics},
language = {eng},
month = {dec},
number = {12},
pages = {e23357},
pmid = {33372664},
title = {{Using Character-Level and Entity-Level Representations to Enhance Bidirectional Encoder Representation From Transformers-Based Clinical Semantic Textual Similarity Model: ClinicalSTS Modeling Study}},
url = {http://medinform.jmir.org/2020/12/e23357/},
volume = {8},
year = {2020}
}
@article{Li2020,
abstract = {Clinical Named Entity Recognition (CNER) is a critical task which aims to identify and classify clinical terms in electronic medical records. In recent years, deep neural networks have achieved significant success in CNER. However, these methods require high-quality and large-scale labeled clinical data, which is challenging and expensive to obtain, especially data on Chinese clinical records. To tackle the Chinese CNER task, we pre-train BERT model on the unlabeled Chinese clinical records, which can leverage the unlabeled domain-specific knowledge. Different layers such as Long Short-Term Memory (LSTM) and Conditional Random Field (CRF) are used to extract the text features and decode the predicted tags respectively. In addition, we propose a new strategy to incorporate dictionary features into the model. Radical features of Chinese characters are used to improve the model performance as well. To the best of our knowledge, our ensemble model outperforms the state of the art models which achieves 89.56% strict F1 score on the CCKS-2018 dataset and 91.60% F1 score on CCKS-2017 dataset.},
author = {Li, Xiangyang and Zhang, Huan and Zhou, Xiao-Hua},
doi = {10.1016/j.jbi.2020.103422},
issn = {15320464},
journal = {Journal of Biomedical Informatics},
keywords = {China,Computer,Electronic Health Records,Neural Networks,Text Messaging},
language = {eng},
month = {jul},
pages = {103422},
pmid = {32353595},
title = {{Chinese clinical named entity recognition with variant neural structures based on BERT methods}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1532046420300502},
volume = {107},
year = {2020}
}
@article{Li2020a,
abstract = {Today, despite decades of developments in medicine and the growing interest in precision healthcare, vast majority of diagnoses happen once patients begin to show noticeable signs of illness. Early indication and detection of diseases, however, can provide patients and carers with the chance of early intervention, better disease management, and efficient allocation of healthcare resources. The latest developments in machine learning (including deep learning) provides a great opportunity to address this unmet need. In this study, we introduce BEHRT: A deep neural sequence transduction model for electronic health records (EHR), capable of simultaneously predicting the likelihood of 301 conditions in one's future visits. When trained and evaluated on the data from nearly 1.6 million individuals, BEHRT shows a striking improvement of 8.0–13.2% (in terms of average precision scores for different tasks), over the existing state-of-the-art deep EHR models. In addition to its scalability and superior accuracy, BEHRT enables personalised interpretation of its predictions; its flexible architecture enables it to incorporate multiple heterogeneous concepts (e.g., diagnosis, medication, measurements, and more) to further improve the accuracy of its predictions; its (pre-)training results in disease and patient representations can be useful for future studies (i.e., transfer learning).},
author = {Li, Yikuan and Rao, Shishir and Solares, Jos{\'{e}} Roberto Ayala and Hassaine, Abdelaali and Ramakrishnan, Rema and Canoy, Dexter and Zhu, Yajie and Rahimi, Kazem and Salimi-Khorshidi, Gholamreza},
doi = {10.1038/s41598-020-62922-y},
issn = {2045-2322},
journal = {Scientific Reports},
keywords = {Algorithms,Electronic Health Records,Humans,Machine Learning},
language = {eng},
month = {dec},
number = {1},
pages = {7155},
pmid = {32346050},
title = {{BEHRT: Transformer for Electronic Health Records}},
url = {http://www.nature.com/articles/s41598-020-62922-y},
volume = {10},
year = {2020}
}
@article{Steinkamp2020,
abstract = {INTRODUCTION: Machine learning (ML) and natural language processing have great potential to improve information extraction (IE) within electronic medical records (EMRs) for a wide variety of clinical search and summarization tools. Despite ML advancements, clinical adoption of real time IE tools for patient care remains low. Clinically motivated IE task definitions, publicly available annotated clinical datasets, and inclusion of subtasks such as coreference resolution and named entity normalization are critical for the development of useful clinical tools. MATERIALS AND METHODS: We provide a task definition and comprehensive annotation requirements for a clinically motivated symptom extraction task. Four annotators labeled symptom mentions within 1108 discharge summaries from two public clinical note datasets for the tasks of named entity recognition, coreference resolution, and named entity normalization; these annotations will be released to the public. Baseline human performance was assessed and two ML models were evaluated on the symptom extraction task. RESULTS: 16,922 symptom mentions were identified within the discharge summaries, with 11,944 symptom instances after coreference resolution and 1255 unique normalized answer forms. Human annotator performance averaged 92.2% F1. Recurrent network model performance was 85.6% F1 (recall 85.8%, precision 85.4%), and Transformer-based model performance was 86.3% F1 (recall 86.6%, precision 86.1%). Our models extracted vague symptoms, acronyms, typographical errors, and grouping statements. The models generalized effectively to a separate clinical note corpus and can run in real time. CONCLUSION: To our knowledge, this dataset will be the largest and most comprehensive publicly released, annotated dataset for clinically motivated symptom extraction, as it includes annotations for named entity recognition, coreference, and normalization for more than 1000 clinical documents. Our neural network models extracted symptoms from unstructured clinical free text at near human performance in real time. In this paper, we present a clinically motivated task definition, dataset, and simple supervised natural language processing models to demonstrate the feasibility of building clinically applicable information extraction tools.},
author = {Steinkamp, Jackson M and Bala, Wasif and Sharma, Abhinav and Kantrowitz, Jacob J},
doi = {10.1016/j.jbi.2019.103354},
issn = {15320464},
journal = {Journal of Biomedical Informatics},
keywords = {Computer,Electronic Health Records,Humans,Information Storage and Retrieval,Machine Learning,Natural Language Processing,Neural Networks},
language = {eng},
month = {feb},
pages = {103354},
pmid = {31838210},
title = {{Task definition, annotated dataset, and supervised natural language processing models for symptom extraction from unstructured clinical notes}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S153204641930276X},
volume = {102},
year = {2020}
}
@article{AgnikulaKshatriya2021,
abstract = {BACKGROUND: There are significant variabilities in guideline-concordant documentation in asthma care. However, assessing clinician's documentation is not feasible using only structured data but requires labor-intensive chart review of electronic health records (EHRs). A certain guideline element in asthma control factors, such as review inhaler techniques, requires context understanding to correctly capture from EHR free text. METHODS: The study data consist of two sets: (1) manual chart reviewed data-1039 clinical notes of 300 patients with asthma diagnosis, and (2) weakly labeled data (distant supervision)-27,363 clinical notes from 800 patients with asthma diagnosis. A context-aware language model, Bidirectional Encoder Representations from Transformers (BERT) was developed to identify inhaler techniques in EHR free text. Both original BERT and clinical BioBERT (cBERT) were applied with a cost-sensitivity to deal with imbalanced data. The distant supervision using weak labels by rules was also incorporated to augment the training set and alleviate a costly manual labeling process in the development of a deep learning algorithm. A hybrid approach using post-hoc rules was also explored to fix BERT model errors. The performance of BERT with/without distant supervision, hybrid, and rule-based models were compared in precision, recall, F-score, and accuracy. RESULTS: The BERT models on the original data performed similar to a rule-based model in F1-score (0.837, 0.845, and 0.838 for rules, BERT, and cBERT, respectively). The BERT models with distant supervision produced higher performance (0.853 and 0.880 for BERT and cBERT, respectively) than without distant supervision and a rule-based model. The hybrid models performed best in F1-score of 0.877 and 0.904 over the distant supervision on BERT and cBERT. CONCLUSIONS: The proposed BERT models with distant supervision demonstrated its capability to identify inhaler techniques in EHR free text, and outperformed both the rule-based model and BERT models trained on the original data. With a distant supervision approach, we may alleviate costly manual chart review to generate the large training data required in most deep learning-based models. A hybrid model was able to fix BERT model errors and further improve the performance.},
author = {{Agnikula Kshatriya}, Bhavani Singh and Sagheb, Elham and Wi, Chung-Il and Yoon, Jungwon and Seol, Hee Yun and Juhn, Young and Sohn, Sunghwan},
doi = {10.1186/s12911-021-01633-4},
issn = {1472-6947},
journal = {BMC Medical Informatics and Decision Making},
keywords = {Algorithms,Asthma,Deep Learning,Electronic Health Records,Humans,Natural Language Processing,diagnosis,drug therapy},
language = {eng},
month = {nov},
number = {S7},
pages = {272},
pmid = {34753481},
title = {{Identification of asthma control factor in clinical notes using a hybrid deep learning model}},
url = {https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-021-01633-4},
volume = {21},
year = {2021}
}
@article{Finch2021,
abstract = {OBJECTIVE: Attention networks learn an intelligent weighted averaging mechanism over a series of entities, providing increases to both performance and interpretability. In this article, we propose a novel time-aware transformer-based network and compare it to another leading model with similar characteristics. We also decompose model performance along several critical axes and examine which features contribute most to our model's performance. MATERIALS AND METHODS: Using data sets representing patient records obtained between 2017 and 2019 by the Kaiser Permanente Mid-Atlantic States medical system, we construct four attentional models with varying levels of complexity on two targets (patient mortality and hospitalization). We examine how incorporating transfer learning and demographic features contribute to model success. We also test the performance of a model proposed in recent medical modeling literature. We compare these models with out-of-sample data using the area under the receiver-operator characteristic (AUROC) curve and average precision as measures of performance. We also analyze the attentional weights assigned by these models to patient diagnoses. RESULTS: We found that our model significantly outperformed the alternative on a mortality prediction task (91.96% AUROC against 73.82% AUROC). Our model also outperformed on the hospitalization task, although the models were significantly more competitive in that space (82.41% AUROC against 80.33% AUROC). Furthermore, we found that demographic features and transfer learning features which are frequently omitted from new models proposed in the EMR modeling space contributed significantly to the success of our model. DISCUSSION: We proposed an original construction of deep learning electronic medical record models which achieved very strong performance. We found that our unique model construction outperformed on several tasks in comparison to a leading literature alternative, even when input data was held constant between them. We obtained further improvements by incorporating several methods that are frequently overlooked in new model proposals, suggesting that it will be useful to explore these options further in the future.},
author = {Finch, Anthony and Crowell, Alexander and Chang, Yung-Chieh and Parameshwarappa, Pooja and Martinez, Jose and Horberg, Michael},
doi = {10.1093/jamiaopen/ooab064},
issn = {2574-2531},
journal = {JAMIA Open},
language = {eng},
month = {jul},
number = {3},
pages = {ooab064},
pmid = {34396057},
title = {{A comparison of attentional neural network architectures for modeling with electronic medical records}},
url = {https://academic.oup.com/jamiaopen/article/doi/10.1093/jamiaopen/ooab064/6348545},
volume = {4},
year = {2021}
}
@article{Mahajan2020,
abstract = {BACKGROUND: Although electronic health records (EHRs) have been widely adopted in health care, effective use of EHR data is often limited because of redundant information in clinical notes introduced by the use of templates and copy-paste during note generation. Thus, it is imperative to develop solutions that can condense information while retaining its value. A step in this direction is measuring the semantic similarity between clinical text snippets. To address this problem, we participated in the 2019 National NLP Clinical Challenges (n2c2)/Open Health Natural Language Processing Consortium (OHNLP) clinical semantic textual similarity (ClinicalSTS) shared task. OBJECTIVE: This study aims to improve the performance and robustness of semantic textual similarity in the clinical domain by leveraging manually labeled data from related tasks and contextualized embeddings from pretrained transformer-based language models. METHODS: The ClinicalSTS data set consists of 1642 pairs of deidentified clinical text snippets annotated in a continuous scale of 0-5, indicating degrees of semantic similarity. We developed an iterative intermediate training approach using multi-task learning (IIT-MTL), a multi-task training approach that employs iterative data set selection. We applied this process to bidirectional encoder representations from transformers on clinical text mining (ClinicalBERT), a pretrained domain-specific transformer-based language model, and fine-tuned the resulting model on the target ClinicalSTS task. We incrementally ensembled the output from applying IIT-MTL on ClinicalBERT with the output of other language models (bidirectional encoder representations from transformers for biomedical text mining [BioBERT], multi-task deep neural networks [MT-DNN], and robustly optimized BERT approach [RoBERTa]) and handcrafted features using regression-based learning algorithms. On the basis of these experiments, we adopted the top-performing configurations as our official submissions. RESULTS: Our system ranked first out of 87 submitted systems in the 2019 n2c2/OHNLP ClinicalSTS challenge, achieving state-of-the-art results with a Pearson correlation coefficient of 0.9010. This winning system was an ensembled model leveraging the output of IIT-MTL on ClinicalBERT with BioBERT, MT-DNN, and handcrafted medication features. CONCLUSIONS: This study demonstrates that IIT-MTL is an effective way to leverage annotated data from related tasks to improve performance on a target task with a limited data set. This contribution opens new avenues of exploration for optimized data set selection to generate more robust and universal contextual representations of text in the clinical domain.},
author = {Mahajan, Diwakar and Poddar, Ananya and Liang, Jennifer J and Lin, Yen-Ting and Prager, John M and Suryanarayanan, Parthasarathy and Raghavan, Preethi and Tsou, Ching-Huei},
doi = {10.2196/22508},
issn = {2291-9694},
journal = {JMIR Medical Informatics},
language = {eng},
month = {nov},
number = {11},
pages = {e22508},
pmid = {33245284},
title = {{Identification of Semantically Similar Sentences in Clinical Notes: Iterative Intermediate Training Using Multi-Task Learning}},
url = {http://medinform.jmir.org/2020/11/e22508/},
volume = {8},
year = {2020}
}
@article{Li2021,
abstract = {BACKGROUND: In recent years, with increases in the amount of information available and the importance of information screening, increased attention has been paid to the calculation of textual semantic similarity. In the field of medicine, electronic medical records and medical research documents have become important data resources for clinical research. Medical textual semantic similarity calculation has become an urgent problem to be solved. OBJECTIVE: This research aims to solve 2 problems-(1) when the size of medical data sets is small, leading to insufficient learning with understanding of the models and (2) when information is lost in the process of long-distance propagation, causing the models to be unable to grasp key information. METHODS: This paper combines a text data augmentation method and a self-ensemble ALBERT model under semisupervised learning to perform clinical textual semantic similarity calculations. RESULTS: Compared with the methods in the 2019 National Natural Language Processing Clinical Challenges Open Health Natural Language Processing shared task Track on Clinical Semantic Textual Similarity, our method surpasses the best result by 2 percentage points and achieves a Pearson correlation coefficient of 0.92. CONCLUSIONS: When the size of medical data set is small, data augmentation can increase the size of the data set and improved semisupervised learning can boost the learning efficiency of the model. Additionally, self-ensemble methods improve the model performance. Our method had excellent performance and has great potential to improve related medical problems.},
author = {Li, Junyi and Zhang, Xuejie and Zhou, Xiaobing},
doi = {10.2196/23086},
issn = {2291-9694},
journal = {JMIR Medical Informatics},
language = {eng},
month = {jan},
number = {1},
pages = {e23086},
pmid = {33480858},
title = {{ALBERT-Based Self-Ensemble Model With Semisupervised Learning and Data Augmentation for Clinical Semantic Textual Similarity Calculation: Algorithm Validation Study}},
url = {http://medinform.jmir.org/2021/1/e23086/},
volume = {9},
year = {2021}
}
@article{Mutinda2021,
abstract = {Background Semantic textual similarity (STS) captures the degree of semantic similarity between texts. It plays an important role in many natural language processing applications such as text summarization, question answering, machine translation, information retrieval, dialog systems, plagiarism detection, and query ranking. STS has been widely studied in the general English domain. However, there exists few resources for STS tasks in the clinical domain and in languages other than English, such as Japanese.},
author = {Mutinda, Faith Wavinya and Yada, Shuntaro and Wakamiya, Shoko and Aramaki, Eiji},
doi = {10.1055/s-0041-1731390},
issn = {0026-1270},
journal = {Methods of Information in Medicine},
language = {eng},
month = {jun},
number = {S 01},
pages = {e56--e64},
pmid = {34237783},
title = {{Semantic Textual Similarity in Japanese Clinical Domain Texts Using BERT}},
url = {http://www.thieme-connect.de/DOI/DOI?10.1055/s-0041-1731390},
volume = {60},
year = {2021}
}
@article{Jiang2020,
abstract = {BACKGROUND: Fever of unknown origin (FUO) is a group of diseases with heterogeneous complex causes that are misdiagnosed or have delayed diagnoses. Previous studies have focused mainly on the statistical analysis and research of the cases. The treatments are very different for the different categories of FUO. Therefore, how to intelligently diagnose FUO into one category is worth studying. OBJECTIVE: We aimed to fuse all of the medical data together to automatically predict the categories of the causes of FUO among patients using a machine learning method, which could help doctors diagnose FUO more accurately. METHODS: In this paper, we innovatively and manually built the FUO intelligent diagnosis (FID) model to help clinicians predict the category of the cause and improve the manual diagnostic precision. First, we classified FUO cases into four categories (infections, immune diseases, tumors, and others) according to the large numbers of different causes and treatment methods. Then, we cleaned the basic information data and clinical laboratory results and structured the electronic medical record (EMR) data using the bidirectional encoder representations from transformers (BERT) model. Next, we extracted the features based on the structured sample data and trained the FID model using LightGBM. RESULTS: Experiments were based on data from 2299 desensitized cases from Peking Union Medical College Hospital. From the extensive experiments, the precision of the FID model was 81.68% for top 1 classification diagnosis and 96.17% for top 2 classification diagnosis, which were superior to the precision of the comparative method. CONCLUSIONS: The FID model showed excellent performance in FUO diagnosis and thus would be a potentially useful tool for clinicians to enhance the precision of FUO diagnosis and reduce the rate of misdiagnosis.},
author = {Jiang, Huizhen and Li, Yuanjie and Zeng, Xuejun and Xu, Na and Zhao, Congpu and Zhang, Jing and Zhu, Weiguo},
doi = {10.2196/24375},
issn = {2291-9694},
journal = {JMIR Medical Informatics},
language = {eng},
month = {nov},
number = {11},
pages = {e24375},
pmid = {33172835},
title = {{Exploring Fever of Unknown Origin Intelligent Diagnosis Based on Clinical Data: Model Development and Validation}},
url = {http://medinform.jmir.org/2020/11/e24375/},
volume = {8},
year = {2020}
}
@article{Percha2021,
abstract = {OBJECTIVE: Clinical registries-structured databases of demographic, diagnosis, and treatment information-play vital roles in retrospective studies, operational planning, and assessment of patient eligibility for research, including clinical trials. Registry curation, a manual and time-intensive process, is always costly and often impossible for rare or underfunded diseases. Our goal was to evaluate the feasibility of natural language inference (NLI) as a scalable solution for registry curation. MATERIALS AND METHODS: We applied five state-of-the-art, pretrained, deep learning-based NLI models to clinical, laboratory, and pathology notes to infer information about 43 different breast oncology registry fields. Model inferences were evaluated against a manually curated, 7439 patient breast oncology research database. RESULTS: NLI models showed considerable variation in performance, both within and across fields. One model, ALBERT, outperformed the others (BART, RoBERTa, XLNet, and ELECTRA) on 22 out of 43 fields. A detailed error analysis revealed that incorrect inferences primarily arose through models' tendency to misinterpret historical findings, as well as confusion based on abbreviations and subtle term variants common in clinical text. DISCUSSION AND CONCLUSION: Traditional natural language processing methods require specially annotated training sets or the construction of a separate model for each registry field. In contrast, a single pretrained NLI model can curate dozens of different fields simultaneously. Surprisingly, NLI methods remain unexplored in the clinical domain outside the realm of shared tasks and benchmarks. Modern NLI models could increase the efficiency of registry curation, even when applied "out of the box" with no additional training.},
author = {Percha, Bethany and Pisapati, Kereeti and Gao, Cynthia and Schmidt, Hank},
doi = {10.1093/jamia/ocab243},
issn = {1527-974X},
journal = {Journal of the American Medical Informatics Association},
language = {eng},
month = {dec},
number = {1},
pages = {97--108},
pmid = {34791282},
title = {{Natural language inference for curation of structured clinical registries from unstructured text}},
url = {https://academic.oup.com/jamia/article/29/1/97/6427462},
volume = {29},
year = {2021}
}
@article{Wan2020,
abstract = {The combination of medical field and big data has led to an explosive growth in the volume of electronic medical records (EMRs), in which the information contained has guiding significance for diagnosis. And how to extract these information from EMRs has become a hot research topic. In this paper, we propose an ELMo-ET-CRF model based approach to extract medical named entity from Chinese electronic medical records (CEMRs). Firstly, a domain-specific ELMo model is fine-tuned on a common ELMo model with 4679 raw CEMRs. Then we use the encoder from Transformer (ET) as our model's encoder to alleviate the long context dependency problem, and the CRF is utilized as the decoder. At last, we compare the BiLSTM-CRF and ET-CRF model with word2vec and ELMo embeddings to CEMRs respectively to validate the effectiveness of ELMo-ET-CRF model. With the same training data and test data, the ELMo-ET-CRF outperforms all the other mentioned model architectures in this paper with 85.59% F1-score, which indicates the effectiveness of the proposed model architecture, and the performance is also competitive on the CCKS2019 leaderboard. ]]>},
author = {Wan, Qian and Liu, Jie and Wei, Luona and Ji, Bin},
doi = {10.3934/mbe.2020197},
issn = {1551-0018},
journal = {Mathematical Biosciences and Engineering},
keywords = {China,Electronic Health Records,Language},
language = {eng},
month = {may},
number = {4},
pages = {3498--3511},
pmid = {32987540},
title = {{A self-attention based neural architecture for Chinese medical named entity recognition}},
url = {http://www.aimspress.com/article/doi/10.3934/mbe.2020197},
volume = {17},
year = {2020}
}
@inproceedings{Johnson2020,
abstract = {The ability of caregivers and investigators to share patient data is fundamental to many areas of clinical practice and biomedical research. Prior to sharing, it is often necessary to remove identifiers such as names, contact details, and dates in order to protect patient privacy. Deidentification, the process of removing identifiers, is challenging, however. High-quality annotated data for developing models is scarce; many target identifiers are highly heterogenous (for example, there are uncountable variations of patient names); and in practice anything less than perfect sensitivity may be considered a failure. As a result, patient data is often withheld when sharing would be beneficial, and identifiable patient data is often divulged when a deidentified version would suffice. In recent years, advances in machine learning methods have led to rapid performance improvements in natural language processing tasks, in particular with the advent of large-scale pretrained language models. In this paper we develop and evaluate an approach for deidentification of clinical notes based on a bidirectional transformer model. We propose human interpretable evaluation measures and demonstrate state of the art performance against modern baseline models. Finally, we highlight current challenges in deidentification, including the absence of clear annotation guidelines, lack of portability of models, and paucity of training data. Code to develop our model is open source, allowing for broad reuse.},
address = {New York, NY, USA},
author = {Johnson, Alistair E W and Bulgarelli, Lucas and Pollard, Tom J},
booktitle = {Proceedings of the ACM Conference on Health, Inference, and Learning},
doi = {10.1145/3368555.3384455},
isbn = {9781450370462},
language = {eng},
month = {apr},
pages = {214--221},
pmid = {34350426},
publisher = {ACM},
title = {{Deidentification of free-text medical records using pre-trained bidirectional transformers}},
url = {https://dl.acm.org/doi/10.1145/3368555.3384455},
volume = {2020},
year = {2020}
}
